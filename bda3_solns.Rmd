---
title: "BDA3 Solutions"
author: "Adam Bartonicek"
date: 'Last updated: `r Sys.Date()`'
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE}

colt <- function(colour, alpha) {
  cc <- col2rgb(colour) / 255
  rgb(cc[1], cc[2], cc[3], alpha)
}

```

# Chapter 2

## 2.1 Binomial: unknown number of hits

We have a $\text{Beta}(4, 4)$ prior, toss a coin ten times, and observe
fewer than 3 heads (but we don't know the exact number of heads).

$$p(\theta) \propto \theta^{4 - 1} (1 - \theta)^{4-1}$$
$$p(y \lvert \theta) = {10 \choose 0} \theta^0 (1 - \theta)^{10} + {10 \choose 1} \theta^1 (1 - \theta)^9 + {10 \choose 2} \theta^2 (1 - \theta)^8$$
$$= (1 - \theta)^{10} + 10\theta (1 - \theta)^9 + 45 \theta^2 (1 - \theta)$$

 

$$p(\theta \lvert y) = [(1 - \theta)^{10} + 10\theta (1 - \theta)^9 + 45 \theta^2 (1 - \theta)] \cdot \theta^3(1 - \theta)^3$$
$$= \theta^3(1 - \theta)^{13} + 10 \theta^4(1 - \theta)^{12} + 45 \theta^5 (1 - \theta)^{11}$$

Plot the posterior:

```{r}

theta <- seq(0, 1, 0.01)

post <- theta^3 * (1 - theta)^13 + 10 * theta^4 * (1 - theta)^12 + 
  45 * theta^5 * (1 - theta)^11 
post <- post/sum(post)

plot(theta, post, type = 'l', col = 'steelblue',
     axes = FALSE, lwd = 2,
     xlab = expression(theta), ylab = 'Density')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')

```

## 2.2 Binomial: posterior predictive expectation

We randomly select one of two coins, with equal probability. One coin
has 0.6 probability of landing heads, the other has 0.4 probability of
landing heads. We toss the coin two times and observe two tails. What's
the posterior expectation of the number of further tosses until we get a
head?

$$P(C_1) = P(C_2) = \frac{1}{2}$$
$$C_1: \; \theta = 0.6; \qquad C_2: \; \theta = 0.4$$
$$p(y = 0 \lvert \theta) = \begin{cases} 0.6^0 (1 - 0.6)^2 = 0.4^2 & \text{if } \theta = 0.6 \\ 0.4^0 (1 - 0.4)^2 = 0.6^2 & \text{if } \theta = 0.4  \end{cases}$$
$$p(\theta_i \lvert y) = \begin{cases} \frac{0.4^2}{0.4^2 + 0.6^2} \approx 0.308 & \text{for } \theta_i = 0.6 \\ 
\frac{0.6^2}{0.4^2 + 0.6^2} \approx 0.682 & \text{for } \theta_i = 0.4  \end{cases}  $$

 

$$E(\tilde y \lvert y) = \sum_{i = 1}^2 \sum_{j=0}^\infty p(\tilde y \lvert \theta_i) p(\theta_i \lvert y) = \sum_{i = 1}^2 p(\tilde y \lvert \theta_i) \sum_{j=0}^\infty j \cdot p(\theta_i \lvert y)$$
$$= \frac{0.4^2}{0.4^2 + 0.6^2} \cdot \sum_{j=0}^\infty j \cdot 0.4 (1 -0.4)^j + \frac{0.4^2}{0.6^2 + 0.6^2} \cdot \sum_{j=0}^\infty j \cdot 0.6 (1 -0.6)^j$$
$$= \frac{0.4^2}{0.4^2 + 0.6^2} \cdot \frac{1}{0.6} + \frac{0.4^2}{0.6^2 + 0.6^2} \cdot \frac{1}{0.4} \qquad \text{(by expectation of Geometric distribution)}$$

```{r}

0.4^2 / (0.4^2 + 0.6^2) * (1 / 0.6) + 
  0.6^2 / (0.4^2 + 0.6^2) * (1 / 0.4) 

```

## 2.3 Normal approximation: simple binomial

Let $Y$ be the number of 6's in 1000 rolls of a fair die.

### 2.3 (a)

Approximate $p(y \lvert \theta)$ using normal distribution:

$$E(Y) = np = 1000 \cdot \frac{1}{6} = 166.66 \bar 6$$
$$Var(Y) = np(1-p) = 1000 \cdot \frac{1}{6} \cdot \frac{5}{6} = 138.88 \bar 8$$

```{r}

mu <- 1000 * 1/6
sigma2 <- 1000 * 1/6 * 5/6
  
y <- 120:220

plot(y, dnorm(y, mu, sqrt(sigma2)), 
     type = 'l', col = 'steelblue',
     axes = FALSE, lwd = 2,
     xlab = 'y', ylab = 'Density')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')

```

### 2.3 (b)

Give approximate 5%, 25%, 50%, 75% and 95% points for the distribution
of $y$

```{r}

qnorm(c(0.05, 0.25, 0.5, 0.75, 0.95), mu, sqrt(sigma2))

```

## 2.4 Normal approximation: binomial mixture

Again, we count the number of 6's in 1000 tosses of a die that may or
may not be fair with the following prior probabilities:

$$p \bigg(\theta = \frac{1}{12} \bigg) = \frac{1}{4}$$
$$p \bigg(\theta = \frac{1}{6} \bigg) = \frac{1}{2}$$
$$p \bigg(\theta = \frac{1}{4} \bigg) = \frac{1}{4}$$ 

### 2.4 (a)

Approximate $p(y \lvert \theta)$ using a normal approximation:

$$p(y \lvert \theta) = \frac{1}{4} \cdot p \bigg(y \lvert \theta = \frac{1}{12} \bigg) + \frac{1}{2} \cdot p \bigg(y \lvert \theta = \frac{1}{6} \bigg) + \frac{1}{4} \cdot p \bigg(y \lvert \theta = \frac{1}{4} \bigg)$$

```{r}

y <- 0:400

theta <- c(1/12, 1/6, 1/4)
mu <- 1000 * theta
sigma2 <- 1000 * theta * (1 - theta)

post <- sapply(y, function(x) dnorm(x, mu, sqrt(sigma2)))
post <- colSums(post * c(1/4, 1/2, 1/4))

plot(y, post, 
     type = 'l', col = 'steelblue',
     axes = FALSE, lwd = 2,
     xlab = 'y', ylab = 'Density')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')

```

### 2.4 (b)

Give approximate 5%, 25%, 50%, 75%, and 95% points for the distribution
of $y$. We can make use of the fact that close to 1/4 of the probability
mass will be in the first normal hump, 1/2 will be in the second normal
hump, and 1/4 will be in the third normal hump (since the humps share
little overlap):

```{r}

q5 <- qnorm(0.2, mu[1], sqrt(sigma2[1]))
q25 <- mu[1] + (mu[2] - mu[1]) / 2
q50 <- qnorm(0.5, mu[2], sqrt(sigma2[2]))
q75 <- mu[2] + (mu[3] - mu[2]) / 2
q95 <- qnorm(0.8, mu[3], sqrt(sigma2[3]))

plot(y, post, 
     type = 'l', col = 'steelblue',
     axes = FALSE, lwd = 2,
     xlab = 'y', ylab = 'Density')
abline(v = c(q5, q25, q50, q75, q95), 
       col = 'grey60', lty = 'dashed')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')


```

## 2.5 Posterior as compromise between prior and data

Let $y$ be number of heads in $n$ spins of a coin with probability of heads $\theta$.

### 2.5 (a)

Assuming a $\text{Uniform}(0, 1)$ prior on $\theta$, derrive the prior predictive distribution:

$$p(y = k) = \int_{0}^1 p(y = k \lvert \theta) d \theta$$
$$= \int_0^1 {n \choose k} \theta^k (1 - \theta)^{n - k} d \theta$$
$$= {n \choose k} \int_0^1 \theta^k (1 - \theta)^{n - k} d \theta \qquad \text{(Beta integral)}$$
$$= \frac{n!}{k! (n - k)!} \cdot \frac{\Gamma(k + 1) \Gamma(n - k + 1)}{\Gamma(n + 2)}$$
$$= \frac{1}{n + 1}$$

### 2.5 (b)

Assuming $\text{Beta}(\alpha, \beta)$ prior, show that the posterior mean lies between prior mean $\frac{\alpha}{\alpha + \beta}$ and the sample mean/MLE $\frac{y}{n}$

The mean of the Beta posterior is $\frac{\alpha + y}{\alpha + \beta + n}$.

$$\frac{\alpha + y}{\alpha + \beta + n} = w \cdot \frac{\alpha}{\alpha + \beta} + (1 - w) \frac{y}{n}$$
$$\implies \frac{\alpha + y}{\alpha + \beta + n} - \frac{y}{n} = w \bigg( \frac{\alpha}{\alpha + \beta} - \frac{y}{n} \bigg)$$
$$\implies \frac{ \alpha n + ny - \alpha y - \beta y - ny}{n(\alpha + \beta + n)} = w \cdot \bigg( \frac{\alpha n - y (\alpha + \beta)}{n(\alpha + \beta)} \bigg)$$
$$\implies \frac{ \alpha n + - \alpha y - \beta y}{n(\alpha + \beta + n)} = w \cdot \bigg( \frac{\alpha n - \alpha y - \beta y}{n(\alpha + \beta)} \bigg)$$
$$\implies w = \frac{\frac{n \alpha + - \alpha y - \beta y}{n(\alpha + \beta + n)}}{\frac{\alpha n - \alpha y - \beta y}{n(\alpha + \beta)}}$$
$$\implies w = \frac{\alpha + \beta}{\alpha + \beta + n}; \qquad (1 - w) = \frac{n}{\alpha + \beta + n}$$
Since $\alpha > 0$, $\beta > 0$ and $n \in \{1, 2, 3, \ldots \}$, $w$ will always be between 0 and 1:

$$0 < w < 1; \qquad 0 < 1 - w < 1$$

Thus, the posterior mean will always lie between the prior mean and the sample mean.

### 2.5 (c)

Show that if the prior is uniform, the posterior variance will always be smaller than the prior variance.

The variance of a $\text{Beta}(\alpha, \beta)$ distribution is: $\frac{\alpha \beta}{(\alpha + \beta)^2 \cdot (\alpha + \beta + 1)}$.

Therefore, with a uniform $\text{Beta}(1, 1)$ prior, the prior variance will be:

$$Var(\theta) = \frac{1 \cdot 1}{(1 + 1)^2 \cdot (1 + 1 + 1)} = \frac{1}{12}$$
The posterior variance will be:

$$Var(\theta \lvert y) = \frac{(1 + y) \cdot (1 + n - y)}{(1 + y + 1 + n -y)^2 (1 + y + 1 + n - y + 1)} = \frac{(1 + y)(1 + n - y)}{(n + 2)^2 (n + 3)}$$
Let's take the extreme case where the data is the least informative it can be: $n = 1$ and $y = 0$ (we could also do $y=1$). Then the posterior variance will be:

$$Var(\theta \lvert y = 0) = \frac{(1 + 0) \cdot (1 + 1 - 0)}{(1 + 2)^2 (1 + 3)} = \frac{2}{36} = \frac{1}{18} < \frac{1}{12}$$
The posterior variance will then be lower than the prior variance.

We can also re-factorize the posterior variance in the following way:

$$Var(\theta \lvert y) = \frac{(1 + y)(1 + n - y)}{(n + 2)^2 (n + 3)} = \bigg( \frac{1 + y}{n + 2} \bigg) \bigg( \frac{1 + n - y}{n + 2} \bigg) \bigg( \frac{1}{n+3} \bigg)$$

$\bigg( \frac{1 + y}{n + 2} \bigg)$ and $\bigg( \frac{1 + n - y}{n + 2} \bigg)$ sum to 1, so their product can be at most $\frac{1}{4}$ (if each is exactly $\frac{1}{2}$). $\frac{1}{n + 3}$ is smaller than or equal to $\frac{1}{4}$, so the posterior variance will be at least $\frac{1}{16} < \frac{1}{12}$.

### 2.5 (d)

Give an example of $\text{Beta}(\alpha, \beta)$ prior and data $y$, $n$, in which posterior variance is higher than prior variance.

```{r}

beta_var <- function(a, b, n, y) {
  ((a + y) * (b + n)) / ((a + b + n)^2 * (a + b + n + 1)) 
}

c(beta_var(1, 5, 0, 0), beta_var(1, 5, 1, 1))

c(beta_var(5, 1, 0, 0), beta_var(5, 1, 1, 1))

c(beta_var(10, 1, 0, 0), beta_var(10, 1, 10, 1))

```

Works for various cases with low values of $y$ and $n$.

## 2.6 Mean and variance of negative binomial predictive distribution

We have:

$$y_j \sim \text{Poisson}(10n_j \theta_j); \qquad \theta_j \sim \text{Gamma}(\alpha, \beta)$$
We're asked to find $E(y_j)$ and $Var(y_j)$. We can use the formulas for conditional mean and variance:

$$E(Y) = E(E(Y \lvert \theta))$$
$$Var(Y) = Var(E(Y \lvert \theta)) + E(Var(Y \lvert \theta))$$
Thus:

$$E(y_j) = E(E(y_j \lvert \theta_j)) = E(10n_j \theta_j) = 10n_j \frac{\alpha}{\beta}$$
$$Var(y_j) = Var(E(y_j \lvert \theta_j)) + E(Var(y_j \lvert \theta_j))$$
$$= Var(10n_j \theta_j) + E(10n_j \theta_j) = (10n_j)^2 \frac{\alpha}{\beta^2} + 10n_j \frac{\alpha}{\beta}$$

## 2.7 Binomial: non-informative prior

### 2.7 (a)

For binomial likelihood, show that $\frac{1}{\theta (1 - \theta)}$ is the uniform prior distribution for the natural parameter of the exponential family. 

$$p(y \lvert \theta) \propto \theta^y (1- \theta)^{n - y}$$
$$= exp(y \cdot log(\theta) - (1 - y) \cdot log(1 - \theta))$$
$$= exp \bigg(y \cdot log \bigg(\frac{\theta}{1 - \theta} \bigg) - (- y \cdot log(1 - \theta))  \bigg)$$
Thus:

$$\phi = \phi(\theta) = log \bigg( \frac{\theta}{1 - \theta} \bigg)$$
$$\implies \theta = \theta(\phi) = \frac{e^{\phi}}{1 + e^{\phi}}$$
$$\frac{\partial \theta}{\partial \phi} = \frac{e^{\phi}(1 + e^{\phi}) - e^{\phi} e^{\phi}}{(1 + e^{\phi})^2} = \frac{e^{\phi}}{(1 + e^{\phi})^2}$$

Thus since $p(\theta) = \frac{1}{\theta (1 - \theta)}$:

$$p(\phi) = p_{\theta}(\theta(\phi)) \cdot \bigg\lvert \frac{\partial \theta}{\partial \phi} \bigg\lvert = \frac{1}{\frac{e^{\phi}}{1 + e^{\phi}} \cdot (1 - \frac{e^{\phi}}{1 + e^{\phi}})} \cdot \frac{e^{\phi}}{(1 + e^{\phi})^2}$$
$$= \frac{1}{\frac{e^{\phi}}{1 + e^{\phi}} \cdot \frac{1}{1 + e^{\phi}}} \cdot \frac{e^{\phi}}{(1 + e^{\phi})^2}$$
$$= \frac{1}{\frac{e^{\phi}}{(1 + e^{\phi})^2}} \cdot \frac{e^{\phi}}{(1 + e^{\phi})^2} = 1$$

### 2.7 (b)

Show that if $n=0$ or $y=0$ the resulting posterior is improper:

$$p(\theta \lvert y) = p(y \lvert \theta) p(\theta) \propto \theta^y (1 - \theta)^{n-y} \cdot \theta^{-1} (1 - \theta)^{-1}$$
$$= \theta^{y - 1}(1 - \theta)^{n - y - 1}$$

In order for the distribution to be proper, it needs to integrate to 1. For $y = 0$ 

$$\int_0^1 \theta^{0-1} (1 - \theta)^{n-0-1} d\theta = \int_0^1 \theta^{-1} (1 - \theta)^{n-1} d \theta = \infty$$
(since the integrand will become infinite as $\theta \to 0$)

Likewise, for $n = 0$:

$$\int_0^1 \theta^{y-1} (1 - \theta)^{0-y-1} d\theta = \int_0^1 \theta^{y-1} (1 - \theta)^{-y-1} d \theta = \infty$$
(again, since the integrand will become infinite as $\theta \to 1$)

## 2.8 Normal distribution with unknown mean

Weights of random sample of $n$ students are recorded, resulting in mean weight $\bar y = 150$. Assume that the weight of the population are distributed around unknown mean $\theta$ with a standard deviation $20$. Assume a $\text{Normal}(180, 40^2)$ prior for $\theta$.

### 2.8 (a)

Give posterior distribution for $\theta$.

Since both the prior and the likelihood are normal, we know that the posterior will be normal with the following mean and variance:

$$E(\theta \lvert y) = \frac{\frac{n \bar y}{\sigma^2} + \frac{\mu_0}{\sigma_0^2}}{\frac{n}{\sigma^2} + \frac{1}{\sigma_0^2}} = \frac{\frac{150n}{20^2} + \frac{180}{40^2}}{\frac{n}{20^2} + \frac{1}{40^2}}$$
$$Var(\theta \lvert y) = \frac{1}{\frac{n}{\sigma^2} + \frac{1}{\sigma_0^2}} = \frac{1}{\frac{n}{20^2} + \frac{1}{40^2}}$$
Thus:

$$\theta \lvert y \sim \text{Normal} \Bigg( \frac{\frac{150n}{20^2} + \frac{180}{40^2}}{\frac{n}{20^2} + \frac{1}{40^2}} ,  \frac{1}{\frac{n}{20^2} + \frac{1}{40^2}}\Bigg)$$

### 2.8 (b)

Give a posterior predictive distribution for a new student $\tilde y$.

The posterior predictive distribution will again be a normal, since it is a product of normals:

$$p(\tilde y) = p(\tilde y \lvert \theta) p( \theta \lvert y) p(\theta)$$
$$E(\tilde y ) = E(E(\tilde y \lvert \theta, y)) = E(\theta \lvert y) = \mu$$
$$Var( \tilde y) = Var(E(\tilde y \lvert \theta, y)) + E(Var( \tilde y \lvert \theta, y)) = Var(\theta \lvert y) + E(\sigma^2 \lvert y)$$
$$= \sigma_n^2 + \sigma^2$$

Therefore, the posterior predictive distribution will be:

$$\tilde y \sim \text{Normal} \Bigg( \frac{\frac{150n}{20^2} + \frac{180}{40^2}}{\frac{n}{20^2} + \frac{1}{40^2}} ,  \frac{1}{\frac{n}{20^2} + \frac{1}{40^2}} + 20^2 \Bigg)$$

### 2.8 (c)

For $n=10$, give 95% credible interval for $\theta$ & 95% posterior predictive interval for $\tilde y$:

```{r}

n <- 10
mu_theta <- (n / 20^2 * 150 + 1/40^2 * 180) / (n/20^2 + 1/40^2)
sigma2_theta <- 1 / (n/20^2 + 1/40^2)
sigma2_yt <- 1 / (n/20^2 + 1/40^2) + 20^2

qnorm(c(0.025, 0.975), mu_theta, sqrt(sigma2_theta))
qnorm(c(0.025, 0.975), mu_theta, sqrt(sigma2_yt))

```

### 2.8 (d)

Do the same for $n = 100$

```{r}

n <- 100
mu_theta <- (n / 20^2 * 150 + 1/40^2 * 180) / (n/20^2 + 1/40^2)
sigma2_theta <- 1 / (n/20^2 + 1/40^2)
sigma2_yt <- 1 / (n/20^2 + 1/40^2) + 20^2

qnorm(c(0.025, 0.975), mu_theta, sqrt(sigma2_theta))
qnorm(c(0.025, 0.975), mu_theta, sqrt(sigma2_yt))

```

The posterior for $\theta$ is a lot more precise, however the posterior predictive distribution does not change much (because the uncertainty about $\theta$ accounted for relatively little compared to the population variance).

## 2.9 Setting parameters for Beta prior

Let $\theta$ be the proportion of Californians who support the death penalty, and our prior for $\theta$ is Beta with a mean 0.6 and standard deviation 0.3.

### 2.9 (a)

Determine the $\alpha$ and $\beta$ parameters of the prior above.

$$E(\theta) = \frac{\alpha}{\alpha + \beta} = 0.6$$
$$Var(\theta) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} = (0.3)^2 = 0.09$$
Thus:

$$\frac{\alpha}{\alpha + \beta} = 0.6 \implies 0.4 \alpha = 0.6 \beta \implies \alpha = \frac{3}{2} \beta$$
&nbsp;

$$\frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} = (0.3)^2 = 0.09$$
$$\implies \frac{\frac{3}{2} \beta^2}{(\frac{5}{2} \beta)^2 (\frac{5}{2} \beta + 1)} = 0.09$$
$$\implies \frac{3}{2} = 0.09 \cdot \frac{25}{4} \cdot \bigg( \frac{5}{2} \beta + 1 \bigg)$$
$$\implies 3 \cdot 50 = 9  \bigg( \frac{125}{8} \beta + \frac{25}{4} \bigg)$$
$$\implies 150 \cdot 8 = 9 \cdot 125 \beta + 18 \cdot 25$$
$$\implies 750 = 1125 \beta$$
$$\implies \beta = \frac{2}{3}$$
$$\implies \alpha = 1$$

### 2.9 (b)

A sample of 1,000 Californians is taken and 65% support the death penalty. What is the posterior mean and variance for $\theta$? Draw the posterior density.

```{r}

a <- 1 + 650
b <- 1 + 1000 - 650

theta <- seq(0, 1, 0.001)

plot(theta, dbeta(theta, a, b), type = 'l',
     xlim = c(0.55, 0.75), col = 'steelblue', lwd = 2,
     xlab = expression(theta), ylab = 'Density', axes = FALSE)
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')

```

## 2.10 Cable car problem

Suppose there are $N$ cable cars in San Francisco, numbered 1 to $N$. You see a cable car at random numbered 203.

### 2.10 (a)

Assume geometric distribution on $N$ with a mean of 100:

$$p(N) = \frac{1}{100} \cdot \bigg(\frac{99}{100} \bigg)^{N-1}$$

What is the posterior distribution?

$$p(N \lvert x = 203) \propto p(X=203 \lvert N) p(N)$$
$$= \frac{1}{N} \cdot \frac{1}{100} \cdot \bigg(\frac{99}{100} \bigg)^{N-1} \qquad \text{(for } N\geq 203)$$
$$\propto \frac{1}{N} \cdot \bigg(\frac{99}{100} \bigg)^{N-1} \qquad \text{(for } N\geq 203)$$

### 2.10 (b)

To be able to evaluate the posterior, we need to compute the normalizing constant $c$ where:

$$\frac{1}{c} = \sum_{i=203}^\infty \frac{1}{N} \bigg( \frac{99}{100} \bigg)^{N-1}$$

We can compute this constant & the posterior over a finite grid of values in R:

```{r}

N <- 203:1000

ci <- sum(1/N * (99/100)^(N-1))
c <- 1 / ci
post <- c * 1/N * (99/100)^(N - 1)

# Compute posterior mean
(post_mean <- sum(N * post))

# Compute posterior variance
(post_variance <- sum((N - post_mean)^2 * post))

plot(N, post, type = 'l',
     col = 'steelblue', lwd = 2,
     xlab = expression(theta), ylab = 'Density', axes = FALSE)
abline(v = post_mean, col = 'grey60', lty = 'dashed')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')

```

### 2.10 (c)

Choose a reasonable "non-informative" prior for $N$ & compute posterior mean and variance.

Using $p(N) = \frac{1}{N}$:

$$p(N \lvert x = 203) \propto \frac{1}{N} \cdot \frac{1}{N} = \frac{1}{N^2}$$
$$E(N \lvert x = 203) = \sum_{N=203}^\infty \frac{1}{N^2}$$
$$= \sum_{N=1}^\infty \frac{1}{N^2} - \sum_{N=1}^{202} \frac{1}{N^2}$$

## 2.11 Computing with a non-conjugate single-parameter model

We observe 5 observations from a Cauchy distribution with unknown center $\theta$: $(43, 44, 45, 46.5, 47.5)$. Assume a $\text{Uniform}(0, 100)$ prior on $\theta$.

### 2.11 (a)

Compute the unnormalized posterior density function on a grid of points. Using the grid approximation, compute and plot the normalized posterior:

```{r}

y <- c(43:45, 46.5, 47.5)
theta <- seq(0, 100, 0.1)

post <- sapply(theta, function(theta) 
  exp(sum(-log((1 + (y - theta)^2)))))
post <- post / sum(post)

plot(theta, post, type = 'l',
     col = 'steelblue', lwd = 2, xlim = c(40, 50),
     xlab = expression(theta), ylab = 'Density', axes = FALSE)
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')

```

### 2.11 (b)

Sample 1,000 draws of $\theta$ from the posterior density and plot a histogram:

```{r}

theta_s <- sample(theta, 1000, prob = post,
                  replace = TRUE)

hist(theta_s, axes = FALSE, main = NULL,
     col = 'steelblue', border = 'white',
     xlab = expression(theta[s]), ylab = 'Count')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')

```

### 2.11 (c)

Use the 1,000 sample of $\theta$ to obtain 1,000 samples from the posterior predictive distribution of a future observation $y_6$ and plot:

```{r}

set.seed(123456)
y6 <- rcauchy(1000, theta_s)

hist(y6, axes = FALSE, main = NULL, breaks = 100,
     col = 'steelblue', border = 'white',
     xlab = expression(y[6]), ylab = 'Count')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')


```

## 2.12 Jeffrey's prior for Poisson distribution

Let $y \lvert \theta \sim \text{Poisson}(\theta)$. Derive Jeffrey's prior for $\theta$ & then find $\alpha$ & $\beta$ parameters for the corresponding Gamma prior distribution:

$$p(y \lvert \theta) = \frac{\theta^y e^{-\theta}}{y!}$$
$$log(p(y \lvert \theta)) = y \cdot log(\theta) - \theta - log(y!) \propto y \cdot log(\theta) - \theta$$
$$\frac{\partial}{\partial \theta} log(p(y \lvert \theta)) = \frac{y}{\theta} - 1$$
$$\frac{\partial^2}{\partial \theta^2} log(p(y \lvert \theta)) = -\frac{y}{\theta^2}$$
$$J(\theta) = E \bigg(- \frac{\partial^2}{\partial \theta^2} log(p(y \lvert \theta)) \bigg) = \frac{\theta}{\theta^2} = \frac{1}{\theta}$$

$$\implies p(\theta) = \sqrt{J(\theta)} = \theta^{-\frac{1}{2}}$$

Thus:

$$\theta \sim \text{Gamma}(1/2, 0)$$

## 2.13 Discrete data: Airplane accidents

We are given number of accidents and deaths on scheduled flights over a 10-year period. 

### 2.13 (a)

Assume that fatal accidents are $\text{Poisson}(\theta)$ distributed. Set a prior distribution, determine the posterior distribution, & give a 95% predictive interval for the number of fatal accidents in 1986.

```{r}

year <- 1976:1985
accidents <- c(24, 25, 31, 31, 22, 21, 26, 20, 16, 22)
deaths <- c(734, 516, 754, 877, 814, 362, 764, 809, 223, 1066)
death_rate <- c(0.19, 0.12, 0.15, 0.16, 0.14, 0.06, 0.13, 0.13, 0.03, 0.15)

theta_s <- rgamma(1e3, sum(accidents), length(accidents))
y1986 <- rpois(1e3, theta_s)

hist(y1986, axes = FALSE, main = NULL,
     col = 'steelblue', border = 'white',
     xlab = 'Predicted number of accidents in 1986', ylab = 'Count')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')

quantile(y1986, c(0.025, 0.975))

```

### 2.13 (b)

Assume that fatal accidents are Poisson distributed with a constant rate and exposure in each year proportional to the number of miles flown. Set a prior distribution, determine the posterior distribution, & give a 95% predictive interval for the number of fatal accidents in 1986.

```{r}

miles <- deaths / death_rate 
theta_s <- rgamma(1e3, sum(accidents), 
                  length(accidents) * mean(miles) / 1e3)
y1986_2 <- rpois(1e3, theta_s * 8)

hist(y1986_2, axes = FALSE, main = NULL,
     col = 'steelblue', border = 'white',
     xlab = 'Predicted number of accidents in 1986', ylab = 'Count')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')

quantile(y1986_2, c(0.025, 0.975))

```

## 2.14 Algebra of the Normal model

### 2.14 (a)

Derive the posterior distribution of a conjugate Normal model:

$$p(y \lvert \theta) \propto exp \bigg(-\frac{1}{2} \bigg( \frac{(y - \mu)^2}{\sigma^2} + \frac{(\mu - \mu_0)^2}{\sigma_0^2} \bigg) \bigg)$$
$$= exp \bigg(-\frac{1}{2} \bigg( \frac{y^2}{\sigma^2} - \frac{2y \mu}{\sigma^2} + \frac{\mu^2}{\sigma^2} + \frac{\mu^2}{\sigma_0^2} - \frac{2 \mu \mu_0}{\sigma_0^2} + \frac{\mu_0^2}{\sigma_0^2} \bigg) \bigg)$$
$$= exp \bigg(-\frac{1}{2} \bigg(\frac{\mu^2}{\sigma^2} + \frac{\mu^2}{\sigma_0^2} - \frac{2y \mu}{\sigma^2} - \frac{2 \mu \mu_0}{\sigma_0^2} + c \bigg) \bigg)$$
$$\propto exp \bigg(-\frac{1}{2} \bigg(\mu^2 \bigg( \frac{1}{\sigma^2} + \frac{1}{\sigma_0^2} \bigg) -  2\mu \bigg( \frac{y }{\sigma^2} - \frac{ \mu_0}{\sigma_0^2} \bigg)\bigg) \bigg)$$
$$\propto exp \Bigg(-\frac{1}{2} \cdot \bigg( \frac{1}{\sigma^2} + \frac{1}{\sigma_0^2} \bigg) \cdot \bigg(\mu  -  \frac{\frac{y}{\sigma^2} - \frac{ \mu_0}{\sigma_0^2}}{\frac{1}{\sigma^2} + \frac{1}{\sigma_0^2}} \bigg)^2 \Bigg) \qquad \text{(by completing the square)}$$

Thus:

$$\mu \lvert y \sim \text{Normal} \Bigg( \frac{\frac{y}{\sigma^2} - \frac{ \mu_0}{\sigma_0^2}}{\frac{1}{\sigma^2} + \frac{1}{\sigma_0^2}}, \frac{1}{\frac{1}{\sigma^2} + \frac{1}{\sigma_0^2}}  \Bigg)$$

### 2.14 (b)

Derive the conjugate Normal model for 2+ observations by adding 1 observation at a time:

We start with a $\mu \lvert y_1 \sim \text{Normal} \Bigg( \frac{\frac{y_1}{\sigma^2} - \frac{ \mu_0}{\sigma_0^2}}{\frac{1}{\sigma^2} + \frac{1}{\sigma_0^2}}, \frac{1}{\frac{1}{\sigma^2} + \frac{1}{\sigma_0^2}}  \Bigg)$ model (after observing 1 observation). Then:

$$\frac{1}{\sigma_2^2} = \frac{1}{\sigma^2} + \frac{1}{\sigma_1^2} = \frac{1}{\sigma^2} + \bigg( \frac{1}{\sigma^2} + \frac{1}{\sigma_0^2} \bigg) = \frac{2}{\sigma^2} + \frac{1}{\sigma_0^2}$$
$$\implies \sigma_2^2 = \frac{1}{\frac{2}{\sigma^2} + \frac{1}{\sigma_0^2}}$$

$$\mu_2 = \frac{\frac{y_2}{\sigma^2} + \frac{\mu_1}{\sigma_1^2}}{\frac{1}{\sigma_2^2}}$$
$$= \frac{\frac{y_2}{\sigma^2} + \bigg( \frac{\frac{y_1}{\sigma^2} + \frac{\mu_0}{\sigma_0^2}}{\frac{1}{\sigma^2} + \frac{1}{\sigma_0^2}} \bigg) \cdot \bigg( \frac{1}{\sigma^2} + \frac{1}{\sigma_0^2} \bigg)}{\frac{1}{\sigma_2^2}}$$
$$= \frac{\frac{y_2}{\sigma^2} + \frac{y_1}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} }{\frac{1}{\sigma_2^2}}$$
$$= \frac{\frac{2 \bar y}{\sigma^2} + \frac{\mu_0}{\sigma^2}}{\frac{2}{\sigma^2} + \frac{1}{\sigma_0^2}}$$

where $\bar y = \frac{y_1 + y_2}{2}$. It is easy to see that this updating formula will work for $n = 2, 3, \ldots$.

## 2.15 Mean and variance of Beta distribution

We have the following result (Beta function):

$$\int_0^1 u^{\alpha - 1} (1 - u)^{\beta - 1} du = \frac{\Gamma(\alpha) \Gamma (\beta)}{\Gamma(\alpha + \beta)}$$

If $Z$ has a Beta distribution, find $E[Z^m (1 - z)^n]$, hence find mean and variance of Beta distribution:

$$E[Z^m (1 - z)^n] = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \int_0^1 z^{\alpha + m -1} (1 - z)^{\beta + n - 1} dz$$
$$= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \frac{\Gamma(\alpha + m) \Gamma(\beta + n)}{\Gamma(\alpha + \beta + m + n)}$$

If we want the expectation of $Z$, $E(Z)$, then clearly $m=1$ and $n = 0$:

$$E(Z) = E[Z^1(1 - Z)^0] = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \frac{\Gamma(\alpha + 1) \Gamma(\beta + 0)}{\Gamma(\alpha + \beta + 1 + 0)}$$
$$= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \frac{\alpha \cdot\Gamma(\alpha) \Gamma(\beta + 0)}{(\alpha + \beta) \cdot \Gamma(\alpha + \beta)}$$
$$= \frac{\alpha}{\alpha + \beta}$$
We can do the same for variance of $Z$:

$$Var(Z) = E(Z^2) - E(Z)^2 = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \frac{\Gamma(\alpha + 2) \Gamma(\beta + 0)}{\Gamma(\alpha + \beta + 2 + 0)} - \bigg( \frac{\alpha}{\alpha + \beta} \bigg)^2$$
$$= \frac{(\alpha + 1)\alpha}{(\alpha + \beta + 1) (\alpha + \beta)} - \frac{\alpha^2}{(\alpha + \beta)^2}$$
$$= \frac{(\alpha^2 + \alpha)(\alpha + \beta) - \alpha^2 (\alpha + \beta + 1)}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$$
$$= \frac{\alpha^3 + \alpha^2 \beta + \alpha^2 + \alpha \beta - \alpha^3 - \alpha^2 \beta - \alpha^2}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$$
$$= \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$$

## 2.16 Beta-binomial distribution

Let $Y$ have a $\text{Binomial}(n, \theta)$ distribution and $\theta$ have a $\text{Beta}(\alpha, \beta)$ prior. 

### 2.16 (a)

Find $p(y)$ i.e. the marginal distribution of $y$:

$$p(y) = \int_{\theta} p(y \lvert \theta) p(\theta) d \theta$$
$$= \int_0^1 {n \choose y} \theta^y  (1 - \theta)^{n - y} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha - 1} (1 - \theta)^\beta - 1{} d \theta$$
$$= {n \choose y} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \int_0^1 \theta^{\alpha + y - 1} (1 - \theta)^{\beta + n-y -1} d \theta$$
$$= {n \choose y} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \frac{\Gamma(\alpha + y) \Gamma(\beta + n - y)}{\Gamma(\alpha + \beta + n)}$$

(i.e. the Beta-Binomial distribution)

### 2.16 (b)

Show that, if the beta-binomial probability is constant in $y$, then $\alpha = \beta = 1$:

$${n \choose y} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \frac{\Gamma(\alpha + y) \Gamma(\beta + n - y)}{\Gamma(\alpha + \beta + n)} \propto \frac{\Gamma(\alpha + y) \Gamma(\beta + n - y)}{y!(n-y)!} \qquad \text{(all terms not involving } y \text{ are constant)}$$
$$= \frac{(\alpha + y - 1)! (\beta + n - y)!}{y! (n - y)!}$$

Clearly, the factorial terms will cancel out if $\alpha = \beta = 1$, thus the probability will be constant in $y$. 

## 2.17 Invariance to transformation

Let $\frac{nv}{\sigma^2}$ be distributed as $\chi^2_n$, and let $\sigma$ have an improper prior distribution with density given by $p(\sigma) \propto \frac{1}{\sigma}$

### 2.17 (a)

Prove that the corresponding prior density for $\sigma^2$ is $p(\sigma^2) \propto \frac{1}{\sigma^2}$.

Let $x = \sigma$ and $y = \sigma^2$.

$$y = y(x) = x^2$$
$$x = x(y) = \sqrt{y}$$
$$\frac{\partial x}{\partial y} = \frac{1}{2 \sqrt{y}}$$

$$\implies p_Y(y) = p_X(x(y)) \cdot \bigg\lvert \frac{\partial x}{\partial y} \bigg\lvert = \frac{1}{\sqrt{y}} \cdot \frac{1}{2 \sqrt{y}}$$
$$= \frac{1}{\sqrt{\sigma^2}} \cdot \frac{1}{2 \sqrt{\sigma^2}} \propto \frac{1}{\sigma^2} \qquad \text{(substituting } y = \sigma^2 \text{ back in)}$$

### 2.17 (b)

Show that hte 95% highest posterior density region for $\sigma^2$ is not the same as the obtained by squaring the endpoints of the posterior interval for $\sigma$.

$$p(\sigma \lvert \text{data}) \propto p(\text{data} \lvert \sigma)p(\sigma) = \frac{1}{2^{\frac{n}{2}} \Gamma(\frac{n}{2})} \bigg( \frac{nv}{\sigma^2} \bigg)^{\frac{n}{2} - 1} exp \bigg(-\frac{nv}{2\sigma^2} \bigg) \cdot \frac{1}{\sigma} \propto (\sigma^2)^{-\frac{n}{2} - \frac{1}{2}} exp \bigg(-\frac{c}{\sigma^2} \bigg)$$
$$p(\sigma^2 \lvert \text{data}) \propto p(\text{data} \lvert \sigma)p(\sigma^2) = \frac{1}{2^{\frac{n}{2}} \Gamma(\frac{n}{2})} \bigg( \frac{nv}{\sigma^2} \bigg)^{\frac{n}{2} - 1} exp \bigg(-\frac{nv}{2\sigma^2} \bigg) \cdot \frac{1}{\sigma^2} \propto (\sigma^2)^{-\frac{n}{2} - 1} exp \bigg(-\frac{c}{\sigma^2} \bigg)$$
$$(\text{where} c = \frac{nv}{2})$$

Let $(\sqrt{a}, \sqrt{b})$ and $(a, b)$ be the 95% highest posterior density intervals for $\sigma$ and $\sigma^2$ respectively. Then:

$$a^{-\frac{n}{2} - \frac{1}{2}} exp\bigg(-\frac{c}{a} \bigg) = b^{-\frac{n}{2} - \frac{1}{2}} exp\bigg(-\frac{c}{b} \bigg)$$
$$a^{-\frac{n}{2} - 1} exp\bigg(-\frac{c}{a} \bigg) = b^{-\frac{n}{2} - 1} exp\bigg(-\frac{c}{b} \bigg)$$

Or equivalently: 

$$\bigg(-\frac{n}{2} - \frac{1}{2} \bigg) log(a) - \frac{c}{a} = \bigg(-\frac{n}{2} - \frac{1}{2} \bigg) log(b) - \frac{c}{b}$$
$$\bigg(-\frac{n}{2} - 1 \bigg) log(a) - \frac{c}{a} = \bigg(-\frac{n}{2} - 1 \bigg) log(b) - \frac{c}{b}$$
$$\implies \frac{1}{2} log(a) = \frac{1}{2} log(b)$$
Which will be true only if $a = b$ but then the 95% credible interval will be just a point containing zero probability mass, and therefore won't be a valid interval & we have a contradiction.

## 2.18 Posterior of Poisson model parametrized with rate and exposure

Let $y$ be distributed as $\text{Poisson}(x_i \lambda)$ where $\lambda$ is the rate parameter and $x_i$ is an exposure variable, and $\lambda$ have a $\text{Gamma}(\alpha, \beta)$ prior distribution. Then:

$$p(\lambda \lvert y) \propto p(y \lvert \lambda)p(\lambda) = \prod_{i=1}^n \frac{(x_i \lambda)^{y_i} e^{-x_i \lambda}}{y_i!} \cdot \frac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{- \beta \lambda}$$
$$\propto \lambda^{ \alpha + \sum_{i = 1}^n  y_i - 1} e^{- (\beta + \sum_{i=1}^n x_i) \lambda}$$
$$\implies \lambda \lvert y \sim \text{Gamma} \bigg(\alpha + \sum_{i=1}^n y_i, \beta + \sum_{i=1}^n x_i \bigg)$$

## 2.19 Conjugate exponential model

### 2.19 (a)

Show that if $y$ is $\text{Exponential}(\theta)$ distributed, then the conjugate prior for $\theta$ is Gamma:

$$p(y \lvert \theta) = \prod_{i= 1}^n \theta e^{-\theta y_i} = \theta^n e^{-\theta\sum_{i = 1}^n y_i}$$
$$p(\theta) = \frac{\beta^\alpha}{\Gamma(\alpha)} \theta^{\alpha - 1} \beta^{\beta - 1}$$

$$\implies p(\theta \lvert y) \propto \theta^{\alpha + n - 1}e^{-(\beta + \sum_{i=1}^n y_i) \theta}$$
$$\implies \theta \lvert y \sim \text{Gamma}(\alpha + n, \beta + \sum_{i=1}^n y_i)$$

### 2.19 (b)

Show that equivalnt prior specification for mean, $\phi = \frac{1}{\theta}$ is Inverse-Gamma.

$$\phi = \phi(\theta) = \frac{1}{\theta}$$
$$\theta = \theta(\phi) = \frac{1}{\phi}$$
$$\frac{\partial \theta}{\partial \phi} = - \frac{1}{\phi^2}$$

$$p_\phi (\phi) = p_\theta(\theta (\phi)) \cdot \bigg\lvert \frac{\partial \theta}{\partial \phi} \bigg\lvert \propto \bigg( \frac{1}{\phi} \bigg)^{\alpha - 1} e^{-\frac{\beta}{\phi}} \cdot \frac{1}{\phi^2} = \phi^{-\alpha - 1} e^{-\frac{\beta}{\phi}}$$
$$\implies \phi \sim \text{Inverse-Gamma}(\alpha, \beta)$$
### 2.19 (c)

The length of a life of a lightbulb is distributed $\text{Exponential}(\theta)$. Suppose $\theta$ has a prior Gamma distribution with coefficient of variation (standard deviation divided by mean) of 0.5. If the coefficient of variation is to be reduced to 0.1, how many lightbulbs are to be tested?

$$\frac{\sqrt{\frac{\alpha}{\beta^2}}}{\frac{\alpha}{\beta}} = 0.5$$
$$\implies \frac{1}{\sqrt{\alpha}} = 0.5$$
$$\implies \frac{1}{\alpha} = 0.25 \implies \alpha = 4$$

The posterior distribution for $\theta$ will be $\text{Gamma}(\alpha + n , \beta + \sum_{i=1}^n y_i)$, therefore:

$$\frac{1}{\sqrt{\alpha + n}} = 0.1$$
$$\implies \frac{1}{\sqrt{4 + n}} = 0.1$$
$$\implies \frac{1}{n + 4} = 0.01$$
$$\implies 100 = n + 4 \implies n = 96$$

Thus we need to test at least 96 lightbulbs.

## 2.20 Censored and uncensored exponential model

Suppose $y$ is $\text{Exponential}(\theta)$ distributed and the prior distribution for $\theta$ is $\text{Gamma}(\alpha, \beta)$. We observe $y \geq 100$ but don't know the exact value of $y$. What is the posterior distribution for $\theta \lvert y \geq 100$ as a function of $\alpha$ and $\beta$? What's the posterior mean and variance?

$$p(y \geq 100 \lvert \theta) = 1 - (1 - e^{-100 \theta}) = e^{-100 \theta}$$
$$p(\theta) \propto \theta^{\alpha - 1} e^{-\beta \theta}$$
$$\implies p(\theta \lvert y \geq 100) = \theta^{\alpha - 1} e^{- (\beta + 100) \theta}$$
$$\implies \theta \lvert y \sim \text{Gamma}(\alpha, \beta + 100)$$

Thus:

$$E(\theta \lvert y) = \frac{\alpha}{\beta + 100}$$
$$Var(\theta \lvert y) = \frac{\alpha}{(\beta + 100)^2}$$

### 2.20 (b)

What is the posterior distribution, the posterior mean and variance if we're told $y=100$?

From question **2.19 (a)**, we know that $\theta$ will have a $\text{Gamma}(\alpha + 1, \beta + 100)$ distribution. Therefore:

$$E(\theta \lvert y) = \frac{\alpha + 1}{\beta + 100}$$
$$Var(\theta \lvert y) = \frac{\alpha + 1}{(\beta + 100)^2}$$

### 2.20 (c)

Explain why posterior variance for $\theta \lvert y = 100$ is greater than that of $\theta \lvert y \geq 100$.

*On average*, the variance of $\theta$ decreases with more information:

$$E(Var(\theta \lvert y) \lvert y \geq 100) \leq Var(\theta \lvert y \geq 100)$$
$y = 100$ is just one possible value & so the variance of its corresponding posterior is not necessarily lower than that of $\theta \lvert y \geq 100$; only averaging across across all possible values $y \geq 100$ is guaranteed to result in lower variance.

## 2.21 Simple hierarchical modeling

The file `pew_research_center_june_elect_wknd_data.dta` has data from Pew Research Centre polls taken during 2008 elections.

### 2.21 (a)

Graph the proportion of liberals in each state vs. Obama vote share

```{r}

dt1 <- haven::read_dta('pew_research_center_june_elect_wknd_data.dta')
dt1$state <- haven::as_factor(dt1$state)
dt1 <- subset(dt1, state != 'hawaii') # Drop Hawaii

dt2 <- read.csv('2008ElectionResult.csv')
dt2 <- subset(dt2, !(state %in% c('Alaska', 'Hawaii'))) # Drop Alaska & Hawaii
dt2$state <- tolower(dt2$state)
dt2[dt2$state == 'district of columbia', 1] <- 'washington dc'

# Calculate percentage of "Very liberal" voters
pct_lib <- tapply(dt1$ideo, dt1$state, function(x) mean(x == 5, na.rm = TRUE))
pct_lib <- pct_lib[!is.na(pct_lib)]
dt3 <- data.frame(state = sort(unique(dt1$state)), pct_lib)
dt3 <- merge(dt3, dt2, all.x = TRUE)
dt3 <- dt3[order(levels(dt3$state)), ]

plot(dt3$pct_lib, dt3$vote_Obama_pct / 100,
     axes = FALSE, type = 'n',
     xlab = 'Proportion of "very liberal" voters',
     ylab = "Obama's vote share", xlim = c(0, 0.12))
text(dt3$pct_lib, dt3$vote_Obama_pct / 100, 
     label = dt3$state)
axis(1, tick = FALSE)
axis(2, tick = FALSE)
box(bty = 'L', col = 'grey60')

```

## 2.22 Prior distributions

A hypothetical study is performed where a random sample of 100 college students is first invited to throw 100 free throws to establish their baseline ability. The students then take 50 practice shots each day for one month and then are invited back to take 100 shots for final measurements. Give three prior distributions for $\theta$, improvement in success probability:

### 2.22 (a)

A non-informative prior.

Since $\theta$ is constrained to lie between -1 and 1 (if a student either makes all 100 free throws on the first go and hits none on the second go or vice versa, respectivelly), then the natural choice for a non-informative prior is a $\text{Uniform}(-1, 1)$.

### 2.22 (b)

A subjective prior based on your best knowledge.

We can assume that most students will probably improve with a month of practice, although it's questionable how much. From my (poor) knowledge of basketball, I'd assume the students to make on average about 10/100 free throws on the first go and about 30/100 throws on the second go. Assuming that, a quick and dirty reasonable prior might be $\text{Normal(0.2, 0.2)}$ i.e. one centered on the expected difference but with a fairly wide margin for a better or worse overall trend, with a very low prior probability of $\theta$ exceeding 1 or -1 (with more effort, we could construct a prior that's actually bounded between those values, such as scaled Beta, but that should not be necessary with 100 participants).

### 2.22 (c)

A weakly informative prior

A good, quick and dirty weakly informative prior might be $\text{Normal}(0, 0.5)$, with a fairly diffuse probability mass across the range of reasonable $\theta$'s. Again, this prior suffers from allowing values of $\theta$ smaller than -1 and greater than 1, however, that again should not be a problem with the amount of data.

# Chapter 3

## 3.1 Multinomial as binomial

Suppose data $(y_1, y_2, \ldots , y_k)$ comes from a $\text{Multinomial}(\theta_1, \theta_2, \ldots, \theta_k)$ distribution, and we have a $\text{Dirichlet}(\alpha_1, \alpha_2, \ldots, \alpha_k)$ prior on the $\theta$'s. Let $\alpha = \frac{\theta_1}{\theta_1 + \theta_2}$.

### 3.1 (a)

Write the marginal posterior distribution of $\alpha$:

$$p(\theta_1, \theta_2 \lvert \mathbf{y}) \propto \theta_1^{\alpha_1 + y_1 -1} \theta_2^{\alpha_2 + y_2 - 1} (1 - \theta_1 - \theta_2)^{\sum_{i \not\in \{1, 2 \}} \alpha_i + y_i - 1}$$

$$\alpha = \alpha(\theta_1, \theta_2) = \frac{\theta_1}{\theta_1 + \theta_2}; \qquad \beta = \beta(\theta_1, \theta_2) = \theta_1 + \theta_2$$
$$\theta_1 = \theta_1(\alpha, \beta) = \alpha \beta; \qquad \theta_2 = \theta_2(\alpha, \beta) = \beta - \alpha \beta = \beta (1 - \alpha)$$
$$\lvert D \lvert = \Bigg\lvert \det \Bigg( \begin{bmatrix} \beta & \alpha \\ -\beta & 1 - \alpha \end{bmatrix} \Bigg) \Bigg\lvert = \lvert \beta (1 - \alpha) - \alpha (- \beta) \lvert = \beta$$
$$\implies p(\alpha, \beta \lvert y) = p_{\theta_1, \theta_2} (\theta_1 (\alpha, \beta), \theta_2(\alpha, \beta)) \cdot \lvert D \lvert$$
$$\propto (\alpha \beta)^{\alpha_1 + y_1 - 1} [\beta (1 - \alpha)]^{\alpha_2 + y_2 - 1} [1 - \alpha \beta - \beta(1 - \alpha)]^{\sum_{i \not\in \{1, 2 \}} \alpha_i + y_i - 1} \cdot \beta$$
$$= \alpha^{\alpha_1 + y_1 - 1} (1 - \alpha)^{\alpha_2 + y_2 - 1} \beta^{\alpha_1 + \alpha_2 + y_1 + y_2 - 1} (1 - \beta)^{\sum_{i \not\in \{1, 2 \}} \alpha_i + y_i - 1}$$

Thus:

$$p(\alpha \lvert y) \propto \int_{\beta} p(\alpha, \beta \lvert y) d\beta$$
$$= \alpha^{\alpha_1 + y_1 - 1} (1 - \alpha)^{\alpha_2 + y_2 - 1} \int_0^1 \beta^{\alpha_1 + \alpha_2 + y_1 + y_2 - 1} (1 - \beta)^{\sum_{i \not\in \{1, 2 \}} \alpha_i + y_i - 1} d\beta$$
$$\alpha^{\alpha_1 + y_1 - 1} (1 - \alpha)^{\alpha_2 + y_2 - 1} \cdot c$$
$$\implies \alpha \lvert y \sim \text{Beta}(\alpha_1 + y_1, \alpha_2 + y_2)$$

### 3.1 (b)

Show that this distribution is identical to treating $y_1$ as an observation from $\text{Binomial}(y_1 + y_2, \alpha)$:

$$p(\alpha) \propto \alpha^{\alpha_1 - 1} (1 - \alpha)^{\alpha_2 - 1}$$
$$p(y_1 \lvert \alpha) \propto \alpha^{y_1}(1 - \alpha)^{y_2}$$
$$\implies p(\alpha \lvert y_1) \propto \alpha^{\alpha_1 + y_1 - 1}(1 - \alpha)^{\alpha_2 + y_2 - 1}$$

## 3.2 Comparison of multinomial observations

In pre-debate poll of 1988 presidential debate, 294 respondents showed a preference for George W. Bush and 307 showed preference for Michael Dukakis. In the post-debate poll, 288 showed preference for Bush and 332 for Dukakis. 

For $\alpha_j$, $j=1, 2$, let $\alpha_j$ be the proportion of voters who preferred Bush, out of those who had preference for either Bush or Dukakis at poll time $j$. Plot histogram of the posterior density of $\alpha_2 - \alpha_1$. What is the posterior probability that there was a shift towards Bush?

```{r}

a1 <- rbeta(1e4, 295, 308)
a2 <- rbeta(1e4, 289, 333)

hist(a2 - a1, axes = FALSE, main = NULL,
     col = 'steelblue', border = 'white',
     xlab = expression(alpha[1] - alpha[2]), ylab = 'Count')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')

```

There's only around `r round(mean((a2 - a1) > 0), 2)` probability that the post-debate opinion shifted towards Bush.

## 3.3 Estimation of two independent experiments

Measurement of the effect of magnetic fields on the calcium flow in chickens' brains were taken for two groups of chickens. In the control group, the mean of 32 measurements was 1.013 with a standard deviation of 0.24. In the treatment group, the mean of 36 measurements was 1.173 with a standard deviation of 0.2.

### 3.3 (a)

Assuming a uniform prior on $(\mu_c, \mu_t, log(\sigma_c), log(\sigma_t))$ and that the samples were taken from a normal distribution, what are the posterior distributions of $\mu_c$ and $\mu_t$?

$$\mu_c \lvert \mathbf{y} = t_{31}(1.013, 0.24^2 / 32)$$
$$\mu_t \lvert \mathbf{y} = t_{35}(1.173, 0.20^2 / 36)$$

### 3.3 (b)

What is the posterior distribution for $\mu_t - \mu_c$? Plot histogram & give approximate 95% credible interval. 

```{r}

mu_c <- (0.24 / sqrt(32)) * rt(1e4, 31) + 1.013
mu_t <- (0.2 / sqrt(36)) * rt(1e4, 35) + 1.173 

mu_diff <- mu_t - mu_c

mu_diff_ci <- quantile(mu_diff, c(0.025, 0.975))

hist(mu_diff, axes = FALSE, main = NULL,
     breaks = 20, col = 'steelblue', border = 'white',
     xlab = expression(mu[t] - mu[c]), ylab = 'Count')
abline(v = mu_diff_ci, lty = 'dashed', col = 'grey60')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')

```

## 3.4 Inference for a 2 x 2 table

Patients received either treatment or control. Out of 674 patients receiving the control, 39 died, out of the 680 patients receiving treatment, 22 died. Assume that the outcomes are Binomially distributed, with probabilities $p_0$ and $p_1$ for control and treament groups respectively.

### 3.4 (a)

Set up a non-informative prior distribution on $(p_0, p_1)$ and obtain posterior samples.

```{r}

# Sample thetas from independent Beta distributions
# with non-informative Beta(1, 1) priors
theta0_1 <- rbeta(1e4, 40, 675)
theta1_1 <- rbeta(1e4, 23, 680)

or1 <- (theta1_1 / (1 - theta1_1)) / (theta0_1 / (1 - theta0_1))

# Sample thetas from independent Beta distributions
# with a Jeffrey's Beta(1/2, 1/2) prior 
theta0_2 <- rbeta(1e4, 39.5, 674.5)
theta1_2 <- rbeta(1e4, 22.5, 679.5)

or2 <- (theta1_2 / (1 - theta1_2)) / (theta0_2 / (1 - theta0_2))

hist(or1, axes = FALSE, main = NULL,
     breaks = seq(0, 2, 0.05), 
     col = colt('steelblue', 0.5), border = 'white',
     xlab = 'Odds ratio', ylab = 'Count')
hist(or2, axes = FALSE, main = NULL,
     breaks = seq(0, 2, 0.05), 
     col = colt('indianred', 0.25), border = 'white',
     xlab = 'Odds ratio', ylab = 'Count', add = TRUE)
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')

quantile(or1, c(0.025, 0.975))
quantile(or2, c(0.025, 0.975))

```

### 3.4 (c)

Discuss sensitivity of your inference to your choice of non-informative prior.

The inference doesn't seem to be very strongly affected by either the choice of a "flat", non-informative $\text{Beta}(1, 1)$ prior or a Jeffrey's $\text{Beta}(1/2, 1/2)$ prior.

## 3.5 Rounded data

Let $y = (10, 10, 12, 11, 9)$ be five measurements of an object rounded to the nearest pound. Assume that the unbounded measurements are normally distributed with mean $\mu$ and variance $\sigma^2$.

### 3.5 (c) 

Give the posterior distribution obtained by pretending the observations are exact, unbounded measurements.

```{r}

y <- c(10, 10, 12, 11, 9)
(post_mean <- mean(y))
(s2 <- var(y))
(post_var <- s2 / 5)

```

Assuming a uniform prior on $(\mu, log(\sigma))$, the posterior for $(\mu, \sigma^2)$ will be:

$$p(\mu, \sigma^2 \lvert y) \propto \sigma^{-n - 2} exp \bigg( -\frac{1}{2\sigma^2} [(n-1)s^2 + n(\bar y -\mu) ] \bigg)$$

The marginal posterior for $\mu$ will be:

$$\mu \lvert y \sim t_{4} \bigg(10.4, \frac{1.3}{5} \bigg)$$
The marginal posterior for $\sigma^2$ will be:

$$\sigma^2 \lvert y \sim \text{Inverse-} \chi^2 (4, 1.3)$$

```{r}

# Compute posterior density two different ways:
# 1) Rescale y by post. mean and variance & feed it into dt()
# 2) Draw t(0, 1, 4) samples & rescale by post. mean and variance
mu <- seq(4, 16, 0.01)
post_mu <- dt((mu - 10.4) / sqrt(post_var), 4)
post_mu <- post_mu / sum(post_mu)
mu_s <- sqrt(post_var) * rt(1e4, 4) + 10.4

hi <- hist(mu_s, axes = FALSE, main = NULL, freq = FALSE,
     breaks = 30, col = 'steelblue', border = 'white',
     xlab = expression(mu), ylab = 'Density')
lines(mu, post_mu * (max(hi$density) / max(post_mu)), 
      type = 'l', col = 'indianred', lwd = 2)
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')
legend('topright', lwd = 4, 
       col = c('steelblue', 'indianred'),
       legend = c('Posterior draws', 'Posterior density'))

# The posterior draws, the grid-computed analytic density,
# and a frequentist t-test give roughly the same answers
mu[sapply(c(0.025, 0.975), function(x) 
  min(which(cumsum(post_mu) > x)))]
quantile(mu_s, c(0.025, 0.975))
t.test(y)

# For variance, I'll just plot the histogram of posterior samples
# (could do the same as above though)
sigma2_s <- (4 * s2) / rchisq(1e3, 4) # Sample from Inverse-Chisquare
hist(sigma2_s, axes = FALSE, main = NULL, freq = FALSE,
     breaks = 50, col = 'steelblue', border = 'white',
     xlab = expression(sigma^2), ylab = 'Density')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')

```

### 3.5 (b)

Give the correct posterior distribution for $(\mu, \sigma^2)$ treating the measurements as rounded.

Again, using a noninformative prior density that is uniform on $(\mu, log(\sigma))$:

$$p(\mu, \sigma^2 \lvert y) \propto \frac{1}{\sigma^2} \prod_{i=1}^n \bigg( \Phi \bigg( \frac{y_i + 0.5 - \mu}{\sigma}  \bigg) - \bigg( \frac{y_i - 0.5 - \mu}{\sigma}  \bigg) \bigg)$$

### 3.5 (c)

How do the correct and incorrect posterior distributions differ?

```{r}

mu <- seq(10.4 - 3, 10.4 + 3, length = 100)
log_sigma <- seq(-1, 2, length = 100)

llfun1 <- function(a, b) {
  ll <- 0
  for (i in 1:5) ll <- ll + dnorm(y[i], a, b, log = TRUE)
  return(ll)
}

llfun2 <- function(a, b) {
  ll <- 0
  for (i in 1:5) ll <- ll + log(pnorm(y[i] + 0.5, a, b) -
                                  pnorm(y[i] - 0.5, a, b))
  return(ll)
}

post1 <- outer(mu, exp(log_sigma), llfun1)
post2 <- outer(mu, exp(log_sigma), llfun2)
post1 <- exp(post1 - max(post1))
post2 <- exp(post2 - max(post2))

par(mfrow = c(1, 2))

contour(mu, log_sigma, post1, drawlabels = FALSE, axes = FALSE,
        xlab = expression(mu), ylab = expression(log(sigma)),
        main = 'Unbounded', col = 'steelblue')
box(bty = 'L', col = 'grey60')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
contour(mu, log_sigma, post2, drawlabels = FALSE, axes = FALSE,
        xlab = expression(mu), ylab = expression(log(sigma)),
        main = 'With rounding', col = 'steelblue')
box(bty = 'L', col = 'grey60')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)

```

The two posteriors look largely similar. The posterior that account for rounding has slightly wider range for $log(\sigma)$, suggesting that correctly accounting for rounding introduces more uncertainty into our estimates of the variance of $y$.

### 3.5 (d)

Let $z = (z_1, z_2, z_3, z_4, z_5)$ be the original unbounded measurements. Draw simulations from the posterior distribution and and compute the posterior mean of $(z_2 - z_1)^2$

I decided to tackle the problem using rejection sampling:

```{r}

# First draw imperfect samples from the unbounded posterior
mu_s <- rnorm(1e4, post_mean, sqrt(post_var))
sigma2_s <- (4 * s2) / rchisq(1e3, 4)

p <- runif(1e4)
py <- sum(log(pnorm(y, )))

ll <- sapply(y, function(x) log(pnorm(y + 0.5, mu_s, sigma2_s) -
                            pnorm(y - 0.5, mu_s, sigma2_s)))
lik <- exp(rowSums(ll))
lik <- lik / max(lik)

p <- runif(1e4)

mu_s2 <- mu_s[lik > p]
sigma2_s2 <- sigma2_s[lik > p]

hist(mu_s2)

```

## 3.6 Binomial model with unknown probability and sample size

We observed data $y_i$ coming from a $\text{Binomial}(N, \theta)$ distribution with unknown probability $\theta$ and unknown sample size $N$. Raftery (1988) modelled $N$ as coming from $\text{Poisson}(\mu)$ distribution, with a prior on $\lambda = \mu \theta$.

### 3.6 (a)

Let the prior distribution be $p(\lambda, \theta) = \frac{1}{\lambda}$. What is the motivation for this non-informative prior distribution? Is the distribution proper? Transform to determine $p(N, \theta)$.

The above prior distribution, $p(\lambda, \theta) = \frac{1}{\lambda}$, is a standard non-informative prior distribution that is uniform on $log(\lambda)$. The distribution is clearly improper (since it does not integrate to 1).

To find the prior distribution on $p(N, \theta$, we first need to find $p(\mu, \theta)$ by applying the change of variable technique:

$$\mu = \mu(\lambda, \theta) = \frac{\lambda}{\theta}; \qquad \theta = \theta(\lambda, \theta) = \theta$$
$$\lambda = \lambda(\mu, \theta) = \mu \theta; \qquad \theta = \theta(\mu, \theta) = \theta$$
$$\lvert D \lvert = \begin{vmatrix} \theta & \mu \\ 0 & 1  \end{vmatrix} = \theta$$
$$p(\theta, \mu) = p_{\lambda, \theta}(\lambda(\mu, \theta), \theta(\mu, \theta)) \lvert D \lvert = \frac{1}{\mu \theta} \cdot \theta = \frac{1}{\mu}$$

Hence, by independence: $p(\mu) = \frac{1}{\mu}$. Now to find the prior $p(N, \mu)$:

$$p(N, \mu) = p(N \lvert \mu)p(\mu) = \frac{\mu^N e^{- \mu}}{N!} \cdot \frac{1}{\mu}$$
$$= \frac{\mu^{N - 1} e^{-\mu}}{N!}$$

Finally, to get the prior on $N$, we need to integrate across $\mu$:

$$p(N) = \int_{0}^\infty \frac{\mu^{N - 1} e^{-\mu}}{N!} d \mu$$
$$= \frac{1}{N!} \int_0^\infty \mu^{N - 1} e^{-\mu} d\mu$$
$$= \frac{1}{N!} \cdot \Gamma(N)$$
$$= \frac{(N - 1)!}{N!} = \frac{1}{N}$$

Hence the prior for $(N, \theta)$ will be:

$$p(N, \theta) = \frac{1}{N}$$
And the posterior will be:

$$p(N, \theta \lvert y) \propto \frac{1}{N} \cdot \prod_{i=1}^n {N \choose y_i} \theta^{ y_i} (1 - \theta)^{N - y_i}$$

And the posterior for $N$ is:

$$p(N \lvert y) \propto \frac{1}{N} \Bigg[ \prod_{i=1}^n {N \choose y_i} \Bigg] \int_0^1 \theta^{\sum_{i=1}^n y_i} (1 - \theta)^{\sum_{i=1}^n N - y_i}$$
$$= \frac{1}{N} \Bigg[ \prod_{i=1}^n {N \choose y_i} \Bigg] \cdot \frac{\Gamma(1 + \sum_{i=1}^n y_i) \Gamma(1 + \sum_{i=1}^n N - y_i)}{\Gamma(2 + \sum_{i=1}^n N)}$$
$$= \frac{1}{N} \frac{(\sum_{i=1}^n y_i)! (nN - \sum_{i=1}^ny_i)!}{N(nN + 1)!} \Bigg[ \prod_{i=1}^n {N \choose y_i} \Bigg]$$

### 3.6 (b)

Counts of waterbuck were observed on five separate days in the Kruger Park in South Africa: $y = (53, 57, 66, 67, 72)$. Obtain the posterior for $(N, \theta)$ and display a scatterplot of posterior simulations. What is the posterior probability of $N > 100$?

```{r}

y <- c(53, 57, 66, 67, 72)
N <- max(y):1000
theta <- seq(0.01, 0.99, 0.01)

prior <- outer(N, theta, function(a, b) 1/a)
lpost <- prior

sum_y <- sum(y)

# Calculate the log of the posterior density across the grid
for (i in seq_len(nrow(lpost))) {
  for (j in seq_len(ncol(lpost))) {
    lpost[i, j] <- -log(N[i]) + sum(lchoose(N[i], y)) + 
      sum_y * log(theta[j]) + 
      (5 * N[i] - sum_y) * log(1 - theta[j])
  }
} 

contour(N, theta, exp(lpost), drawlabels = FALSE,
        col = 'steelblue', axes = FALSE,
        xlab = 'N', ylab = expression(theta))
axis(1, tick = FALSE)
axis(2, las = 1, tick = FALSE)
box(bty = 'L', col = 'grey60')

# Marginal posterior density for N
post_N <- rowSums(exp(lpost))
post_N <- post_N / sum(post_N)

plot(post_N, type = 'l', col = 'steelblue',
     lwd = 2, axes = FALSE, 
     xlab = 'N', ylab = 'Posterior density')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')

sum(post_N[N > 100])

```

There is about 95% posterior probability than $N$ is greater than 100.

### 3.6 (c)

Why not simply use a Poisson with fixed $\mu$ as a prior distribution for N?

If we used a $p(N) = \frac{\mu^N e^{-\mu}}{N!}$ prior, then the joint posterior density would not factorize as nicely as above. 

## 3.7 Poisson and binomial distributions

A students sits on a street corner for an hour and records the number of bicycles $b$ and the number of other vehicles $v$. Two models are considered:

- The outcomes $b$ and $v$ have independents Poisson distributions, with unknown means $\theta_b$ and $\theta_v$
- The outcome $b$ has a binomial distribution, with unknown probability $p$ and size $b + v$

Show that the models have the same likelihood if we define $p = \frac{\theta_b}{\theta_b + \theta_v}$:

$$p(b, v \lvert \theta_b, \theta_v) = \frac{\theta_b^b e^{-\theta_b}}{b!} \cdot \frac{\theta_v^v e^{-\theta_v}}{v!} = \frac{\theta_b^b \theta_v^v e^{- \theta_b - \theta_v}}{b! v!}$$

Now apply change of variable:

$$p = p(\theta_b, \theta_v) = \frac{\theta_b}{\theta_b + \theta_v}; \qquad z = z(\theta_b, \theta_v) = \theta_b + \theta_v$$
$$\theta_b = \theta_b (p, z) = pz; \qquad \theta_v = \theta_v (p, z) = z - pz = z(1 - p)$$
$$\lvert D \lvert = \begin{vmatrix} z & p \\ -z & 1 - p \end{vmatrix} = z(1 - p) - p(-z) = z$$
$$\implies p_{p,z }(b, v \lvert p, z) = p_{\theta_b, \theta_v} (\theta_b(p,z) , \theta_v (p,z)) \cdot \lvert D \lvert$$
$$= \frac{(pz)^b [z(1 - p)]^v e^{-zp -z(1 - p)}}{b! v!}$$
$$= \frac{1}{b! v!} p^b z^{b + v} (1 - p)^v e^{-z}$$
$$= \frac{1}{b! v!} p^b (1 - p)^v z^{b + v} e^{-z}$$

Now we integrate across $z$:

$$p(b \lvert p, v) = \frac{1}{b! v!} p^b (1 - p)^v \int_{0}^\infty z^{b + v} e^{-z} dz$$
$$= \frac{1}{b! v!} p^b (1 - p)^v \cdot \Gamma (b + v + 1)$$
$$= \frac{(b + v)!}{b! v!} p^b (1 - p)^v$$
$$= {b + v \choose b} p^b (1 - p)^v = p(b \lvert p, v)$$

## 3.8 Analysis of proportions

A survey was done in which sixty city blocks were selected at random, each block was observed for an hour, an the number of bicycles and other vehicles were recorded. Data from street with and without bike routes were compared. 

```{r}

w_lanes_bic <- c(16, 9, 10, 13, 19, 20, 18, 17, 35, 55)
w_lanes_all <- c(58, 90, 48, 57, 103, 57, 86, 112, 273, 64)
wo_lanes_bic <- c(12, 1, 2, 4, 9, 7, 9, 8)
wo_lanes_all <- c(113, 18, 14, 44, 208, 67, 29, 154)

```

### 3.8 (a)

Let $y_1, y_2, \ldots, y_{10}$ and $z_1, z_2, \ldots, z_{8}$ be the observed proportions of bicycles out of overall traffic on streets with and without lanes. Set up the model so that $y_i$'s and $z_i$'s are independent and identically distributed with parameters $\theta_y$ and $\theta_z$

$$y_i \sim \text{Poisson}(a_i \theta_y)$$
$$z_i \sim \text{Poisson}(b_i \theta_z)$$

$$\implies p(y_i \lvert \theta_y, a_i) = \frac{(a_i \theta_y)^{y_i} e^{- a_i \theta_y}}{y_i!}; \qquad p(z_i \lvert \theta_z, b_i) = \frac{(b_i \theta_z )^{z_i} e^{- b_i \theta_z}}{z_i!}$$

### 3.8 (b)

Set up a prior distribution that is independent in $\theta_y$ and $\theta_z$:

$$p(\theta_y, \theta_z) = p(\theta_y) \cdot p(\theta_z) = \frac{1}{\theta_y} \cdot \frac{1}{\theta_z}$$

### 3.9 (c)

Determine the posterior and draw 1,000 simulations from the posterior distribution:

$$p(\theta_y, \theta_z \lvert y, z, a, b) = p(\theta_y \lvert y, a) \cdot p(\theta_z \lvert z, b)$$
$$= \frac{1}{\theta_y} \cdot \prod_{i=1}^{10} \frac{(a_i \theta_y)^{y_i} e^{- a_i \theta_y}}{y_i!} \cdot \frac{1}{\theta_z} \prod_{i=1}^8 \frac{(b_i \theta_z)^{z_i} e^{-b_i \theta_z}}{z_i!}$$
$$\propto \bigg( \theta_y^{\sum_{i=1}^{10} y_i -1} e^{- \theta_y \sum_{i=1}^{10} a_i} \bigg) \bigg( \theta_z^{\sum_{i=1}^8 z_i}  e^{- \theta_z \sum_{i=1}^8 b_i} \bigg)$$

```{r}

theta_y <- rgamma(1e3, sum(w_lanes_bic), sum(w_lanes_all))
theta_z <- rgamma(1e3, sum(wo_lanes_bic), sum(wo_lanes_all))

```

### 3.8 (d)

Let $\mu_y = E(y_i \lvert \theta_y)$ be the mean of the distribution of $y_i$'s and $\mu_z = E(z_i \lvert \theta_z)$ be the mean of the distribution of $z_i$'s. Use the posterior simulations from **(c)** to compute $\mu_y$ and $\mu_z$ and plot a histogram of $\mu_y - \mu_z$.

```{r}

mu_y <- sapply(theta_y, function(x) 
  mean(rpois(10, x * w_lanes_all)))
mu_z <- sapply(theta_z, function(x) 
  mean(rpois(8, x * wo_lanes_all)))

hist(mu_y - mu_z, main = NA, breaks = 20,
     axes = FALSE, col = 'steelblue', border = 'white',
     xlab = expression(mu[y] - mu[z]), ylab = 'Count')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')

```

## 3.9 Conjugate Normal model

Let $y_i \sim \text{Normal}(\mu, \sigma^2)$ and the prior distribution on $(\mu, \sigma^2)$ be $\text{N-Inv-}\chi^2 (\mu_0,\sigma_0^2 / \kappa_0; \nu_0, \sigma^2_0)$. The posterior for $(\mu, \sigma^2)$ is also Normal-Inverse-$\chi^2$: derive its parameters in terms of prior parameters and sufficient statistics.

$$p(\mu, \sigma^2) \propto \sigma^{-1} exp \bigg(-\frac{\kappa_0}{2 \sigma^2}(\mu - \mu_0)^2 \bigg) \cdot  (\sigma^2)^{- (\nu_0/2 + 1)} exp \bigg(-\frac{1}{2 \sigma^2} (\nu_0 \sigma_0^2) \bigg)$$
$$= \sigma^{-1} (\sigma^2)^{- (\nu_0 /2 + 1)} exp \bigg(-\frac{1}{2\sigma^2} [\nu_0 \sigma_0^2 + \kappa_0 (\mu_0 - \mu)^2] \bigg) $$

$$p(\mu, \sigma^2 \lvert y) = p(y \lvert \mu, \sigma^2) p(\mu, \sigma^2)$$
$$\propto (\sigma^2)^{-\frac{n}{2}} exp \bigg(-\frac{(n - 1)s^2 + n(\mu - \bar y)^2}{2\sigma^2} \bigg) \cdot \sigma^{-1} (\sigma^2)^{- (\nu_0 /2 + 1)} exp \bigg(-\frac{1}{2\sigma^2} [\nu_0 \sigma_0^2 + \kappa_0 (\mu_0 - \mu)^2] \bigg) $$
$$\propto= \sigma^{-1}(\sigma^2)^{-\frac{n + \nu_0}{2} - 1} exp \bigg( \frac{\nu_0 \sigma_0^2 + (n - 1)s^2 + \mu^2(n + \kappa_0) - 2\mu(n \bar y + \kappa_0 \mu_0) + n \bar y^2 + \kappa_0 \mu_0^2 }{2 \sigma^2} \bigg)$$
$$\propto= \sigma^{-1}(\sigma^2)^{-\frac{n + \nu_0}{2} - 1} exp \bigg( \frac{\nu_0 \sigma_0^2 + (n - 1)s^2 + \frac{n \kappa_0 (\bar y - \mu_0)^2}{n + \kappa_0} + (n + \kappa_0)(\mu - \frac{n \bar y + \kappa_0 \mu_0}{n + \kappa_0})^2}{2 \sigma^2} \bigg) \qquad \text{(completing the square)}$$
Thus:

$$\mu, \sigma^2 \lvert y \sim \text{N-Inv-}\chi^2 \bigg( \frac{n \bar y + \kappa_0 \mu_0}{n + \kappa_0}, \frac{\sigma_n^2}{n + \nu_0}; n + \nu_0, \sigma^2_n \bigg)$$
where:

$$\sigma^2_n = \frac{\nu_0 \sigma_0^2 + (n-1)s^2 + \frac{n \kappa_0 (\bar y - \mu_0)^2}{n + \kappa_0}}{n + \nu_0}$$


# Chapter 4

## 4.1 Normal approximation: Cauchy

We observe 5 independent observations from Cauchy distribution with an
unknown parameter $\theta$:
$(y_1, \ldots, y_5) = (-2, -1, 0, 1.5, 2.5)$.

### 4.1 (a)

Determine the first and second derivative of log-posterior density:

$$p(\mathbf{y} \lvert \theta) = \prod_{i=1}^5 \frac{1}{1 + (y_i - \theta)^2}$$

$$log(p(\mathbf{y} \lvert \theta)) = \sum_{i=1}^5 log \bigg( \frac{1}{1 + (y_i - \theta)^2} \bigg) = -\sum_{i=1}^5 log(1 + (y_i - \theta)^2)$$

 

$$\frac{\partial}{\partial \theta} log(p(\mathbf{y} \lvert \theta)) = -\sum_{i=1}^5 \frac{\partial}{\partial \theta} log(1 + (y_i - \theta)^2)$$
$$= - \sum_{i=1}^5 \frac{1}{1 + (y_i - \theta)^2} \cdot 2(y_i - \theta) \cdot(-1)$$
$$= 2 \sum_{i=1}^5 \frac{y_i - \theta}{1 + (y_i - \theta)^2}$$

 

$$\frac{\partial^2}{\partial \theta^2} log(p(\mathbf{y} \lvert \theta)) = 2 \sum_{i=1}^5 \frac{\partial}{\partial \theta} \frac{y_i - \theta}{1 + (y_i - \theta)^2}$$
$$= 2 \sum_{i=1}^5 \frac{(-1)[1 + (y_i - \theta)^2] - 2 \cdot (y_i - \theta) \cdot (-1) \cdot (y_i - \theta)}{[1 + (y_i - \theta)^2]^2}$$
$$= 2 \sum_{i=1}^5 \frac{(y_i - \theta)^2 - 1}{[1 + (y_i - \theta)^2]^2}$$

### 4.1 (b)

To find the posterior mode, we can use numerical optimization:

```{r}

y <- c(-2, -1, 0, 1.5, 2.5)

scorefun <- function(theta) {
  if (theta < 0 || theta > 1) return(Inf)
  2 * sum((y - theta) / (1 + (y - theta)^2)^2)
}

mode <- uniroot(scorefun, c(0, 1))$f.root

```

### 4.1 (c)

Calculate the normal approximation:

```{r}

I <- -2 * sum(((y - mode)^2 - 1) / (1 + (y - mode)^2)^2)
var_theta <- 1 / I

theta <- seq(0, 1, 0.01)

post_true <- sapply(theta, function(x) exp(-sum(log(1 + (y - x)^2))))
post_true <- post_true / sum(post_true)
post_approx <- dnorm(theta, mode, sqrt(var_theta))
post_approx <- post_approx / sum(post_approx)

plot(theta, post_approx, type = 'l', 
     axes = FALSE, lwd = 2, col = 'firebrick',
     xlab = expression(theta), ylab = 'Density')
lines(theta, post_true, lwd = 2, col = 'steelblue', type = 'l')
axis(1, tick = FALSE)
box(bty = 'L', col = 'grey60')

```

## 4.2 Normal approximation: Bioassay

We have four observations from four independent experiments:

$$\mathbf{y} = (0, 1, 3, 5)$$ $$\mathbf{n} = (5, 5, 5, 5)$$
$$\mathbf{x} = (-0.86, -0.3, -0.05, 0.73)$$

$$y_i \lvert \theta_i \sim \text{Binomial}(n_i, \theta_i)$$
$$log \bigg( \frac{\theta_i}{1 - \theta_i} \bigg) = \alpha + \beta x_i \implies \theta_i = \frac{e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}}$$

The likelihood is:

$$p(\mathbf{y} \lvert \alpha, \beta) = \prod_{i=1}^4 {5 \choose y_i} \bigg( \frac{e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}} \bigg)^{y_i} \bigg( 1 - \frac{e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}} \bigg)^{5 - y_i}$$
$$= \prod_{i=1}^4 {5 \choose y_i} \bigg( \frac{e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}} \bigg)^{y_i} \bigg( \frac{1}{1 + e^{\alpha + \beta x_i}} \bigg)^{5 - y_i}$$

$$log(p(\mathbf{y} \lvert \alpha, \beta)) \propto \sum_{i=1}^4 y_i \cdot (\alpha + \beta x_i) - y_i \cdot log(1 + e^{\alpha + \beta x_i}) - 5 \cdot log(1 + e^{\alpha + \beta x_i}) + y_i \cdot log(1 + e^{\alpha + \beta x_i})$$
$$= \sum_{i=1}^4 y_i \cdot (\alpha + \beta x_i) - 5 \cdot log(1 + e^{\alpha + \beta x_i})$$

 

$$\frac{\partial}{\partial \alpha} log(p(y_i \lvert \alpha, \beta)) = y_i - 5 \cdot \frac{e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}}$$
$$\frac{\partial}{\partial \beta} log(p(y_i \lvert \alpha, \beta)) =y_i x_i - 5 \cdot \frac{x_i \cdot e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}}$$

 

$$\frac{\partial^2}{\partial \alpha^2} log(p(y_i \lvert \alpha, \beta)) = - 5 \cdot \frac{e^{\alpha + \beta x_i}(1 + e^{\alpha + \beta x_i}) - e^{\alpha + \beta x_i} \cdot e^{\alpha + \beta x_i}}{(1 + e^{\alpha + \beta x_i})^2}$$
$$= - \frac{5e^{\alpha + \beta x_i}}{(1 + e^{\alpha + \beta x_i})^2}$$

$$\frac{\partial^2}{\partial \alpha \partial \beta} log(p(y_i \lvert \alpha, \beta)) = - 5 \cdot \frac{x_i e^{\alpha + \beta x_i} (1 + e^{\alpha + \beta x_i}) - x_i e^{\alpha + \beta x_i} e^{\alpha + beta x_i}}{(1 + e^{\alpha + \beta x_i})^2}$$
$$= -\frac{5x_i e^{\alpha + \beta x_i}}{(1 + e^{\alpha + \beta x_i})^2}$$

$$\frac{\partial^2}{\partial \beta^2} log(p(y_i \lvert \alpha, \beta)) = - 5 \cdot \frac{x_i^2e^{\alpha + \beta x_i}(1 + e^{\alpha + \beta x_i}) - x_ie^{\alpha + \beta x_i} \cdot x_ie^{\alpha + \beta x_i}}{(1 + e^{\alpha + \beta x_i})^2}$$
$$= -\frac{5x_i^2 e^{\alpha + \beta x_i}}{(1 + e^{\alpha + \beta x_i})^2}$$

Therefore:

$$I(\hat \theta) = \begin{bmatrix} -\sum_{i=1}^n \frac{5e^{\alpha + \beta x_i}}{(1 + e^{\alpha + \beta x_i})^2} & -\sum_{i=1}^n \frac{5x_ie^{\alpha + \beta x_i}}{(1 + e^{\alpha + \beta x_i})^2} \\ -\sum_{i=1}^n \frac{5x_ie^{\alpha + \beta x_i}}{(1 + e^{\alpha + \beta x_i})^2} & -\sum_{i=1}^n \frac{5x_i^2e^{\alpha + \beta x_i}}{(1 + e^{\alpha + \beta x_i})^2}   \end{bmatrix}$$

To find the posterior mode, we can use numerical optimization:

```{r}

y <- c(0, 1, 3, 5)
x <- c(-0.86, -0.3, -0.05, 0.73)

llfun <- function(theta) {
  if(any(theta < 0)) return(Inf)
  a <- theta[1]; b <- theta[2]
  
  -sum(y * (a + b * x) - 5 * log(1 + exp(a + b * x)))
  
}

mode <- optim(c(1, 1), llfun)$par

a <- mode[1]
b <- mode[2]
# Second derivaties evaluated at mode
plpa2 <- -sum( (5 * exp(a + b * x)) / (1 + exp(a + b * x))^2)
plpab <- -sum( (5 * x * exp(a + b * x)) / (1 + exp(a + b * x))^2)
plpb2 <- -sum( (5 * x^2 * exp(a + b * x)) / (1 + exp(a + b * x))^2)

I <- matrix(c(plpa2, plpab, plpab, plpb2), ncol = 2)
var_theta <- -solve(I)

# Draw samples from the approximate posterior
draws_approx <- MASS::mvrnorm(2e3, mode, var_theta)

# Compute the true posterior across a grid of values
anew <- seq(-3, 8, 0.025)
bnew <- seq(-10, 35, 0.1)

post_true <- matrix(nrow = length(anew),
                    ncol = length(bnew))

for (i in seq_along(anew)) {
  for (j in seq_along(bnew)) {
    ps <- exp(anew[i] + bnew[j] * x) / (1 + exp(anew[i] + bnew[j] * x))
    post_true[i, j] <- prod(dbinom(y, 5, ps))
    
  }
}

contour(anew, bnew, post_true, col = 'firebrick',
        axes = FALSE, drawlabels = FALSE,
        xlab = expression(alpha), ylab = expression(beta))
points(draws_approx[, 1], draws_approx[, 2], 
     pch = 19, col = colt('steelblue', 0.05))
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')
legend('topright', col = c('firebrick', 'steelblue'),
       legend = c('True posterior', 'Normal approximation (samples)'),
       lwd = 1, border = 'grey60')

```

## 4.3 Delta method: Bioassay

From the previous question, we have:

```{r}

# Posterior mode
mode

# Posterior variance
var_theta

```

Let $\mathbf{\theta}$ be the parameter vector $(\alpha, \beta)$. Now the function we want to approximate is LD50:

$$\text{LD50} = g(\mathbf{\theta}) = - \frac{\alpha}{\beta}$$
By the invariance of MLE's, we know that:

$$\text{MLE}(g(\mathbf{\theta})) = g( \mathbf{\hat \theta}) = -\frac{\hat \alpha}{\hat \beta}$$
Now, as for the Delta method, we can expand $g(\mathbf{\theta})$ as a Taylor series around the posterior mode:

$$g(\mathbf{\hat \theta}) \approx g(\mathbf{\theta}) + \nabla g(\mathbf{\theta})^T( \mathbf{\hat \theta} - \mathbf{\theta})$$
where $g(\theta)$ is the transformation of the true parameter, and $\nabla g(\theta)$ is the Jacobian of the transformation. We can use this to obtain the variance of the transformed parameters:

$$Var(g(\mathbf{\theta})) \approx Var(g(\mathbf{\theta}) + \nabla g(\mathbf{\theta})^T( \mathbf{\hat \theta} - \mathbf{\theta}))$$
$$= Var(g(\mathbf{\theta}) + \nabla g(\mathbf{\theta})^T \mathbf{\hat \theta} - \nabla g(\mathbf{\theta})^T \mathbf{\theta})$$
$$= Var(\nabla g(\mathbf{\theta})^T \mathbf{\hat \theta}) \qquad \text{(since } g(\mathbf{\theta}) \text{is a constant)}$$
$$= \nabla g(\mathbf{\theta})^T \Sigma g(\mathbf{\theta}) \nabla g(\mathbf{\theta})$$
Additionally:

$$\nabla g(\mathbf{\theta}) = \begin{bmatrix} \frac{\partial}{\partial \alpha} g(\mathbf{\theta}) \\ \frac{\partial}{\partial \beta} g(\mathbf{\theta}) \end{bmatrix} = \begin{bmatrix} -\frac{1}{\beta} \\ \frac{\alpha}{\beta^2} \end{bmatrix} $$

We approximate this with the MLE values:

$$\nabla g(\mathbf{\hat \theta}) = \begin{bmatrix} -\frac{1}{\beta} \\ \frac{\alpha}{\beta^2} \end{bmatrix} \approx \begin{bmatrix} -\frac{1}{7.75} \\ \frac{0.85}{(7.75)^2} \end{bmatrix}$$
Therefore:

$$Var(\mathbf{\hat \theta}) \approx \begin{bmatrix} -\frac{1}{7.75} & \frac{0.85}{(7.75)^2} \end{bmatrix} \begin{bmatrix} 1.04 & 3.55 \\ 3.55 & 2.75 \end{bmatrix} \begin{bmatrix} -\frac{1}{7.75} \\ \frac{0.85}{(7.75)^2} \end{bmatrix}$$

Compute this numerically in R:

```{r}

jacob <- c(-1/mode[2], mode[1] / mode[2]^2)

mode_ld50 <- - mode[1] / mode[2] 
var_ld50 <- as.numeric(t(jacob) %*% var_theta %*% jacob)

(waldci_ld50 <- mode_ld50 + c(-1, 0, 1) * 1.96 * sqrt( var_ld50))

```

The Wald confidence interval seems to roughly correspond to the histogram on page 87.

## 4.4 Asymptotic Normality

Assuming regularity conditions, we know that $p(\theta \lvert y)$ approaches normality as $n \to \infty$. In addition, if $\phi= f(\theta)$ is any one-to-one continuous transformation of $\theta$, we can express the Bayesian inference in terms of $\phi$ and find that $p(\phi \lvert y)$ also approaches normality. But a non-linear transformation is no longer normal. How can both limiting normal distributions be valid?

As $n \to \infty$, posterior variance approaches zero, and so $p(\theta \lvert y)$ will converge to a single point. Any one-to-one continuous transformation will be locally linear in the neighbourhood of that point.

## 4.5 Approximate mean and variance of $\frac{Y}{X}$

Let $X$ and $Y$ be independent normally distributed random variables where $X$ has mean 4 and standard deviation 1, and $Y$ has mean 3 and standard deviation 2. What is the approximate mean and variance of $\frac{Y}{X}$? 

### (a)

Compute using simulation:

```{r}

set.seed(1234)
x <- rnorm(1e4, 4, 1)
y <- rnorm(1e4, 3, 2)

z <- y/x

hist(z, axes = FALSE, main = NULL,
     col = 'steelblue', border = 'white',
     xlab = expression(y/x), ylab = 'Count')
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')

mean(z)
var(z)

```

### (b)

Determine without using simulation.

To approximate the ratio, we have to use a (bivariate) Taylor series approximation. We have the function:

$$f(x, y) = \frac{Y}{X}$$

The derivatives of this function with respect to $X$ and $Y$ are:

$$f_x' = -\frac{Y}{X^2}; \qquad f_x'' = \frac{2X}{Y^3}$$
$$f_{xy}'' = f_{yx}'' = -\frac{1}{X^2}$$ 
$$f_y' = \frac{1}{X}; \qquad f_y'' = 0$$
A first order Taylor series approximation of $f(x, y)$ about the joint mean/centroid $\mathbf{\mu} = (\mu_X, \mu_Y)$ is:

$$f(x, y) \approx f(\mathbf{\mu}) + f'_x(x - \mu_X) + f_y'(y - \mu_Y)$$
If we take an expectation of this, we get:

$$E[f(x, y)] \approx E[f(\mathbf{\mu}) +  f'_x(\mathbf{\mu})(x - \mu_X) + f_y'(\mathbf{\mu})(y - \mu_Y)]$$
$$= E[f(\mathbf{\mu})] +  f'_x(\mathbf{\mu}) E[(x - \mu_X)] + f_y'(\mathbf{\mu}) E[(y - \mu_Y)]$$
$$E[f(\mathbf{\mu})] + 0 + 0$$
$$= f(\mathbf{\mu}) = \frac{\mu_Y}{\mu_X}$$

However, we can do better. A second order Taylor series approximation of the function $f(x, y)$ is:

$$f(x, y) \approx f(\mathbf{\mu}) + f'_x(\mathbf{\mu})(x - \mu_X) + f_y' (\mathbf{\mu})(y - \mu_Y) + \frac{1}{2} \bigg[ f_{xx}''(\mathbf{\mu}) (x - \mu_X)^2 + 2 f_{xy}(\mathbf{\mu}) (x - \mu_X)(y - \mu_Y) + f_{yy}''(\mathbf{\mu})(y \ mu_y)^2 \bigg]$$

Again, taking the expectation, we get:

$$E[f(x, y)] \approx E[f(\mathbf{\mu})] + 0 + 0 + \frac{1}{2} \cdot \frac{2\mu_Y}{\mu_X^3} \cdot E[(x - \mu_X)^2] + \bigg( - \frac{1}{\mu_X^2} \bigg) \cdot E[(x - \mu_X)(y - \mu_Y)] + 0 \cdot E[(y - \mu_Y)^2]$$
$$= \frac{\mu_Y}{\mu_X} + \frac{\mu_Y}{\mu_X^3} Var(X) - \frac{1}{\mu_X^2} Cov(X, Y)$$

Since the two variables are independent, $Cov(X, Y) = 0$ and so:

$$E \bigg( \frac{Y}{X} \bigg) = \frac{\mu_Y}{\mu_X} + \frac{\mu_Y}{\mu_X^3} Var(X) = \frac{3}{4} + \frac{3}{64} \cdot 1^2 \approx 0.797$$

Which is pretty close to the answer we got from the simulation above.

As for variance, technically $Var \bigg( \frac{Y}{X} \bigg) = \infty$ as $n \to \infty$, if the denominator $X$ assigns nonzero probability for values close to zero, which it does since $X$ is a normally distributed. However, that shouldn't bother us too much if the mean of $X$ is far from zero and the standard deviation is small, meaning that we are unlikely to get those troublesome values near zero. Therefore, we can approximate variance with the Delta method (i.e. a Taylor series shortcut):

$$Var \bigg( \frac{Y}{X} \bigg) = \mathbf{G V G^T}$$

where $\mathbf{G}$ is the Jacobian matrix:

$$\mathbf{G} = \begin{bmatrix} -\frac{Y}{X^2} \\ \frac{1}{X} \end{bmatrix} \approx \begin{bmatrix} -\frac{3}{16} \\ \frac{1}{4} \end{bmatrix}$$
$$\implies Var \bigg( \frac{X}{Y} \bigg) \approx \begin{bmatrix} -\frac{3}{16} & \frac{1}{4} \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 4 \end{bmatrix} \begin{bmatrix} -\frac{3}{16} \\ \frac{1}{4} \end{bmatrix} \approx 0.285$$

```{r}

G <- c(-3/16, 1/4)
V <- matrix(c(1, 0, 0, 4), ncol = 2)

approx_mean <- 3/4 + 3/64
approx_var <- t(G) %*% V %*% G

znew <- seq(min(z), max(z), length.out = 100)

hist(z, axes = FALSE, main = NULL, freq = FALSE,
     col = 'steelblue', border = 'white',
     xlab = expression(y/x), ylab = 'Count', ylim = c(0, 0.75))
lines(znew, dnorm(znew, approx_mean, sqrt(approx_var)), 
      col = 'firebrick', lwd = 2)
axis(1, tick = FALSE)
axis(2, tick = FALSE, las = 1)
box(bty = 'L', col = 'grey60')

```

The Taylor approximation seems to work pretty well. The variance is slightly underestimated, as could be expected due to the non-linearity of the transformation. 

## 4.6 Statistical decision theory

A decision-theoretic approach to estimation of an unknown parameter $\theta$ is to introduce a loss function $L(\theta, a)$ which gives the cost of deciding that a parameter has cost $a$ when it is in fact equal to $\theta$. The estimate can be chosen to minimize expected loss:

$$E(L(\theta, a)) = \int L(\theta, a) p(\theta \lvert y) d\theta$$
This optimal choice for $a$ is called the Bayes estimate for loss function $L$. Show that:

### (a)

If $L(\theta, a) = (\theta - a)^2$ (squared error loss), then the posterior mean, $E(\theta \lvert y)$, if it exists, is the unique Bayes estimate for $\theta$.

$$\frac{\partial}{\partial a} \int L(\theta, a) p(\theta \lvert y) d \theta = \frac{\partial}{\partial a} \int (\theta - a)^2 p(\theta \lvert y) d \theta$$
$$= \int \frac{\partial}{\partial a} (\theta - a)^2 p(\theta \lvert y) d \theta$$
$$= \int 2(\theta - a) (-1) p(\theta \lvert y) d \theta$$
$$= \int 2(a -  \theta ) p(\theta \lvert y) d \theta$$
$$= 2 \cdot \int a \cdot p(\theta \lvert y) d \theta - 2 \cdot \int \theta \cdot p (\theta \lvert y ) d \theta$$
$$= 2a \cdot \int p(\theta \lvert y) d \theta - 2 E(\theta \lvert y) = 2a - 2E(\theta \lvert y)$$
Clearly, this will be zero when $a$ is the posterior mean $E(\theta \lvert y)$.

### (b)

If $L(\theta, a) = \lvert \theta - a \lvert $, then the posterior median of $\theta$ is the Bayes estimate of $\theta$.

Using the answer from the following question, **(c)**:

$$k_0 = k_1 = 1$$
$$\implies \frac{k_0}{k_0 + k_1} = \frac{1}{1 + 1} = \frac{1}{2}$$

Thus, the $a$ that minimizes the loss function is the 50th percentile of the posterior cumulative density function, i.e. the median. 

### (c)

If $k_0$ and $k_1$ are non-negative numbers, both nonzero, and:

$$L(\theta, a) = \begin{cases} 
k_0(\theta - a) & \text{if } \theta \geq a \\ 
k_1(a - \theta) & \text{if } \theta < a
\end{cases}$$

$$\frac{\partial}{\partial a} E(L(a \lvert y)) = \frac{\partial}{\partial a} \Bigg[ \int_a^\infty k_0 (\theta - a) p(\theta \lvert y) d \theta + \int_{-\infty}^a k_1(a - \theta) p(\theta \lvert y) d \theta \Bigg]$$
$$= \int_a^\infty \frac{\partial}{\partial a} k_0 (\theta - a) p(\theta \lvert y) d \theta + \int_{-\infty}^a \frac{\partial}{\partial a} k_1(a - \theta) p(\theta \lvert y) d \theta$$
$$= - k_0 \int_{a}^\infty p(\theta \lvert y) d \theta + k_1 \int_{-\infty}^ a p(\theta \lvert y) d \theta$$
$$= k_1 \int_{-\infty }^ a p(\theta \lvert y) d \theta - k_0 \bigg[1 - \int_{-\infty}^a p(\theta \lvert y) d \theta \bigg]$$
$$= (k_0 + k_1) \int_{-\infty}^a p(\theta \lvert y) d \theta - k_0$$

Now set to zero:

$$ (k_0 + k_1) \int_{-\infty}^a p(\theta \lvert y) d \theta = k_0$$
$$\implies \int_{-\infty}^a p(\theta \lvert y) d \theta = \frac{k_0}{k_0 + k_1}$$

Therefore, $a$ is the $\frac{k_0}{k_0 + k_1}$th quantile of the posterior distribution.

## 4.7 (Un)biasedness of the posterior mean

Show that the posterior mean, based on a proper prior distribution, cannot be an unbiased estimator except in degenerate problems.

Let the posterior mean be $m(y) = E(\theta \lvert y)$. For it to be unbiased estimator of $\theta$, the following would have to hold: $E( m(y) \lvert \theta) = \theta$. However, then we'd have:

$$E(\theta m(y) ) = E[E(\theta m(y) \lvert \theta)] = E[\theta \cdot E(m(y) \lvert \theta)] = E[\theta^2]$$
$$E(\theta m(y) ) = E[E(\theta m(y) \lvert y)] = E[m(y) \cdot E( \theta \lvert y)] = E[m(y)^2]$$
Now, the variance of the estimator will be:

$$E[(m(y) - \theta)^2] = E[m(y)^2] - 2 E[m(y)] E[\theta] + E[\theta^2]$$
$$= E[m(y)^2] - 2 \theta^2 + \theta^2$$
$$= E[m(y)^2] - \theta^2 = E[m(y)^2] - E[\theta^2]$$

Using the above, we'd get $E[m(y)^2] - E[\theta^2] = E(\theta m(y) ) - E(\theta m(y) ) = 0$

So if the posterior mean is unbiased as an estimator, its variance has to be zero. This is only the case when we have either infinite amount of data or other degenerate cases. So in all other scenarios, the posterior mean won't be unbiased. 

## 4.8 Regression to the mean

Let the height of mothers and daughters be normally distributed with known means of 160, equal variances, and correlation of 0.5. The posterior mean of $\theta$ is:

$$E(\theta \lvert y) = 160 + 0.5 (y - 160)$$
and given daughter's height, mother's height is distributed as:

$$E(y \lvert \theta) = 160 + 0.5(\theta - 160)$$
So the expectation of the posterior mean is:

$$E[E(\theta \lvert y)] = E[160 + 0.5(y - 160)]$$
$$= 160 + 0.5 \cdot [E(y) - 160]$$
$$= 160 + 0.5 [160 + 0.5(\theta - 160) - 160]$$
$$= 160 + 0.25(\theta - 160)$$
As such, the expectation of the posterior mean is shrunk towards the grand mean of 160 and is therefore biased. In contrast, an estimate:

$$\hat \theta = 160 + 2(y - 160)$$

is unbiased since:

$$E[\hat \theta] = E[160 + 2(y - 160)]$$
$$= 160 + 2 \cdot E[y] - 2 \cdot 160$$
$$= 2 \cdot [160 + 0.5(\theta - 160)] - 160$$
$$= 2 \cdot [80 + 0.5 \theta] - 160 = \theta$$
However, even though unbiased, this estimate clearly doesn't make sense since it estimates the parameter to be more extreme than the data: e.g. for a mother who's 10 centimeters taller than average, it will estimate that the daughter is 20 centimeters taller than average (i.e. the exact opposite of regression to the mean). This shows a downside to frequentist estimation, where an unknown quantity has to be treated differently depending on whether it's a "parameter" or a "prediction".



# Miscallaneous

## M.1 Student-t posterior from normal data & non-informative prior

Let:

$$p(\mu, \sigma^2 ) \propto \frac{1}{\sigma^2} \qquad \text{(i.e. uniform prior on } (\mu, log(\sigma))$$
and:

$$p(\mathbf{y} \lvert \mu, \sigma^2) \propto \frac{1}{\sigma^n} exp \bigg( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2 \bigg) $$

(i.e. class normal model with independent observations)

Then:

$$p(\mu, \sigma^2 \lvert \mathbf{y}) \propto \sigma^{-n - 2}  exp \bigg( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2 \bigg)$$
$$= \sigma^{-n - 2}  exp \bigg( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \bar y + \bar y - \mu)^2 \bigg)$$
$$= \sigma^{-n - 2}  exp \bigg( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \bar y)^2 + (\bar y - \mu)^2 \bigg)$$
$$= \sigma^{-n - 2}  exp \bigg( -\frac{1}{2\sigma^2} [(n-1)s^2 + n(\bar y - \mu)^2] \bigg)$$

Where $\bar y$ and $s^2$ are sample mean and variable, respectively. Now:

$$p(\mu \lvert \mathbf{y}) = \int_{\sigma_2} p(\mu, \sigma^2 \lvert \mathbf{y}) d\sigma^2$$
$$= \int_0^\infty \sigma^{-n - 2}  exp \bigg( -\frac{1}{2\sigma^2} [(n-1)s^2 + n(\bar y - \mu)^2] \bigg)$$

Let $A = [(n-1)s^2 + n(\bar y - \mu)^2]$ and $z = \frac{A}{2 \sigma^2}$. Then:

$$\frac{1}{2 \sigma^2} = \frac{z}{A} = zA^{-1}; \qquad \frac{1}{\sigma} \propto \frac{\sqrt{z}}{\sqrt{A}} = z^{\frac{1}{2}}A^{-\frac{1}{2}}; \qquad \frac{\partial \sigma^2}{\partial z} \propto z^{-\frac{3}{2}} A^{-\frac{1}{2}}$$

Thus:

$$p(\mu \lvert \mathbf{y}) \propto \int_0^\infty (z^{\frac{1}{2}} A^{-\frac{1}{2}})^{n + 2} exp \bigg( \frac{z}{A} \cdot A \bigg) \cdot z^{-2} A \; dz$$
$$= A^{-\frac{n}{2}} \int_0^\infty z^{\frac{n}{2} - 1} exp(-z) dz$$
$$= A^{-\frac{n}{2}} \cdot c \qquad \text{(recognizing the above as Gamma integral)}$$
$$= [(n - 1)s^2 + n(\bar y - \mu)^2]^{-\frac{n}{2}}$$
$$= \bigg[1 + \frac{n(\bar y - \mu)^2}{(n-1)s^2} \bigg]^{-\frac{n}{2}}$$

Therefore:

$$\mu \lvert \mathbf{y} \sim t_{n-1} \bigg(\bar y, \frac{s^2}{n} \bigg)$$